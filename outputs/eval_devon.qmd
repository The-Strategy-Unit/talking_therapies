---
title: "NHS Talking Therapies"
subtitle: "Evaluation of interventions to improve adherence to therapy in *Talkworks, Devon*"
author: "Craig Parylo"
date: today

title-block-banner: '#151412'
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    embed-resources: true
    fig-format: png
    fig-width: 10
    fig-height: 3
    fig-dpi: 600
    default-image-extension: svg
    lightbox: true
    editor: visual
    favicon: "favicon.ico"
    include-in-header:
      text: |
        <link rel="icon" type="image/x-icon" href="favicon.ico">
brand: _brand.yml
css: styles.css
bibliography: references.bib
---

```{r}
#| label: setup
#| context: setup
#| cache: false

# set up params (NB, the ones in YAML don't work well when running interactively)
params <- list(
  "ods_intervention" = "RWV",
  "download_from_teams" = FALSE
)

# load in utility functions
source(here::here('scripts', 'utility_functions.R'))

# reference files ---
# get a list of projects that have implemented something to improve adherence
# part 1 - update the list of intervention projects (doesn't need updating everytime)
if (params$download_from_teams) {
  download_file_from_channel(
    str_path = "2. Project delivery/Project plan/TT projects.xlsx",
    str_dest = here::here(
      'data',
      'project',
      'tt_intervention_projects_copy.xlsx'
    )
  )
}
# part 2
df_intervention_services <- read_an_open_excel(
  path = here::here('data', 'project', 'tt_intervention_projects_copy.xlsx'),
  sheet = "intervention_projects"
)

# get a summary of interventions
df_interventions <- read_an_open_excel(
  path = here::here('data', 'project', 'tt_interventions_summary.xlsx'),
  sheet = 'interventions'
) |>
  dplyr::filter(iapt_code == params$ods_intervention) |>
  dplyr::mutate(calc_month = zoo::as.yearmon(month))

# load in matching variables
df <- get_matching_variables()

# load in variable labels
ls_labels <- get_variable_labels()

```

# Executive summary

The NHSE Behavioural Science team commissioned the Strategy Unit (SU) to deliver an independent evaluation of locally implemented interventions aimed at increasing engagement with the NHS Talking Therapies programme. The main research question is:

> Are these interventions effective (versus doing nothing) at reducing the proportion of clients who complete a course of treatment (attend at least two treatment sessions), but then end treatment after only two, three or four treatment sessions?

**Key findings**

-   **No robust, consistent effect** was found for either a) the proportion of discharged referrals receiving five treatment contacts, or b) the proportion of referrals achieving reliable recovery.

-   **Possible modest positive signals**: point-estimates are slightly above zero, but confidence intervals are wide and the results are not statistically conclusive.

**Implications**

Scale the pilot to allow for a more rigorous experimental design. This should involve a cohort of similar Talking Therapy services, randomising half to receive the interventions at the same time and the other half serving as controls. Follow outcomes prospectively for both groups to obtain precise estimates of effect size.

**Methods**

Difference-in-Differences (DiD) quasi-experimental study design comparing Devon's outcomes pre- and post-intervention with a) a set of services matched with Devon based on six covariates and b) a synthetic control based on a large set of donor services.

Data was sourced from the Increasing Access to Psychological Therapies (IAPT) dataset, a pseudonmyised record-level dataset, accessed via the NHS England's Unified Data Access Layer (UDAL) platform.

The core assumption is that pre-intervention trends of the matched cohort and synthetic control approximate the counterfactual trend Devon would have followed without the interventions.

**Limitations and risks**

-   Devon is the only service to receive the interventions which results in significant uncertainty in the treatment effect

-   Devon's rollout of the interventions was staggered across twelve months, making it difficult to define a clean pre- / post-treatment split and isolate the treatment effect

-   Two control services experienced abrupt shocks in mid-2023 which distorted the post-implementation average performance, weakening the credibility of the estimated trajectory for Devon had they not implemented the interventions.

# Project overview

## Interventions

This service implemented several improvements to patient information designed to better prepare people before they begin therapy.

```{r}
#| label: List of interventions
#| cache: true

df_interventions |>
  dplyr::select(month, intervention) |>
  dplyr::arrange(month) |>
  gt::gt() |>
  gt::tab_options(quarto.disable_processing = TRUE) |>
  gt::fmt_date(columns = month, date_style = "yMMM") |>
  gt::cols_label(month = "Month", intervention = "Intervention") |>
  gt::cols_width(month ~ gt::pct(15)) |>
  gt::tab_header(
    title = "Interventions",
    subtitle = "Changes made by the service to better prepare clients for therapy"
  )
```

```{r}
#| label: Timeline of interventions
#| cache: true

# create a df holding details about the timeline
df_timeline <-
  df_interventions |>
  # make fields compatible with {timevis}
  dplyr::rename(content = intervention_html, start = month) |>
  # select essential details
  dplyr::select(content, start)

# display as a timeline
timevis::timevis(data = df_timeline, showZoom = FALSE)

```

<br> *Timeline view of interventions* <br>

In addition, the service implemented the following changes (without definite start dates, so are not included in this evaluation):

-   Added a [patient and staff testimonial](https://www.talkworks.dpt.nhs.uk/about-us/testimonials) page on the website to help reduce anxiety for people nervous about therapy

-   Began asking self-referred patients who are difficult to contact whether they want to book an assessment, instead of automatically allocating appointments

-   Created a "What to expect from therapy" guide explaining what therapy will look like, how it can be delivered and what to do if someone cannot attend their appointment

## Outcomes

Two outcome measures have been used in this evaluation:

1.  The proportion of discharged referrals where the patient attended five or more treatment sessions

2.  The proportion of discharged referrals that have completed treatment, defined as at least two treatment contacts, but where the patient did not attend at least five treatment sessions.

These measures follow the numerator and denominator definitions, below.

+--------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Item               | Definition                                                                                                                                                                                                                                                            |
+====================+=======================================================================================================================================================================================================================================================================+
| Measure            | The proportion of discharged referrals each month where the patient attended five or more treatment sessions.                                                                                                                                                         |
+--------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Numerator          | Those in the denominator who attended five or more treatment care contacts during their referral[^1]                                                                                                                                                                  |
+--------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Denominator        | Referrals ending each month where the patient was 'seen and taken on for a course of treatment'[^2]                                                                                                                                                                   |
+--------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Rationale          | This denominator represents monthly discharges for referrals that began a course of therapy (feasibility testing showed this group is 43% of all IAPT referrals).                                                                                                     |
|                    |                                                                                                                                                                                                                                                                       |
|                    | The numerator is the subset who attended at least five treatment contacts. Recent research identifies five sessions as a minimum associated with greater likelihood of achieving reliable recovery, so this group are those more likely to achieve reliable recovery. |
|                    |                                                                                                                                                                                                                                                                       |
|                    | This measure therefore captures discharged referrals each month that demonstrate sufficient adherence to have at least 50% likelihood of reliable recovery.                                                                                                           |
+--------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Expected variation | We expect this measure to **increase** as a result of interventions that improve adherence with therapy.                                                                                                                                                              |
+--------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

: Outcome measure 1

[^1]: This count will be obtained from the field ‘TreatmentCareContact_Count’ (object I101D29) from the table ‘IDS101Referral’

[^2]: Defined as an ‘EndCode’ (object I101250 on the table ‘IDS101Referral’) as either:

    -   46 (mutually agreed completion of treatment),

    -   47 (termination of treatment earlier than Care Professional planned),

    -   48 (termination of treatment earlier than the patient requested),

    -   49 (deceased, seen and taken on for a course of treatment) or

    -   96 (not known, seen and taken on for a course of treatment)

    See <https://digital.nhs.uk/binaries/content/assets/website-assets/isce/dcb1520/1520132021tos.xlsx> \[Accessed 5th September 2025\]

+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Item               | Description                                                                                                                                                                  |
+====================+==============================================================================================================================================================================+
| Measure            | The proportion of referrals that have completed a course of therapy but attended fewer than five treatment sessions                                                          |
+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Numerator          | Of the denominator, those who attended between 2 and 4 treatment care contacts (inclusive) in total, during their referral                                                   |
+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Denominator        | Referrals ending each month where the patient was 'seen and taken on for a course of treatment' and attended at least two treatment sessions.                                |
+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Rationale          | This denominator captures monthly discharges who began and completed a course of therapy (IAPT KPI definition: at least two treatment contacts).                             |
|                    |                                                                                                                                                                              |
|                    | Research indicates five sessions is the minimum associated with a greater likelihood of reliable recovery, so this group is most likely to benefit from additional sessions. |
|                    |                                                                                                                                                                              |
|                    | This measure therefore identifies discharged referrals who completed treatment but would likely improve their chance of reliable recovery by attending more sessions.        |
+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Expected variation | We expect this measure to **decrease** as a result of interventions that improve adherence with therapy.                                                                     |
+--------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

: Outcome measure 2

## Matching variables

Since no consensus exists on predictors of therapy adherence, a [literature search](https://github.com/The-Strategy-Unit/talking_therapies/blob/main/data/reference/lit_search.xlsx) identified plausible variables. These were shortlisted based on feasibility of measurement in the IAPT dataset.

The selected factors, listed below, are mostly expressed as proportions of either referrals or care contacts. Aggregating patient-level data into these proportions represents the overall Talking Therapies service and makes the variables more suitable for the matching process.

Outcome measure:

-   Proportion of referrals that have completed treatment, but where the patient did not attend at least five treatment sessions.

Demographic characteristics:

-   Proportion of referrals for people aged 25 years and younger at referral

-   Proportion of referrals for people aged 60 years and older at referral

-   Proportion of referrals for people whose gender identity is female

-   Proportion of referrals for people whose LSOA of residence is among the 20% most deprived in England

-   Proportion of referrals for people whose LSOA of residence is among the 20% least deprived in England

-   Proportion of referrals for people whose broad ethnic background is 'White'

Therapist and service characteristics:

-   Proportion of care contacts where the therapist has attained an NHS TT qualification

-   Proportion of care contacts conducted on hospital premises

-   Proportion of care contacts conducted face-to-face

-   Proportion of care contacts outside of weekdays, 9am to 5pm

-   Proportion of care contacts delivered as internet enabled therapy

-   Proportion of care contacts delivered to an individual patient

-   Proportion of referrals where the referral-to-treatment wait time was within six weeks

-   Proportion of referrals where there was step-up to high-intensity therapy

Language and accessibility:

-   Proportion of care contacts conducted in English

-   Proportion of care contacts conducted with an interpreter present

A draft version of these matching variables was shared by email with members of the NHS Talking Therapies (TT) and Individual Placement and Support (IPS) National Programme Delivery Group for review in July 2025. The list above has since been augmented based on feedback.

## Outcomes for Talkworks, Devon

Talkworks Devon IAPT referral discharges data is available from the beginning of 2022 with the most recent data point being June 2025.

```{r}
#| label: Devon discharges time series
#| fig-height: 7
#| cache: true

df |>
  dplyr::filter(
    ods_code == params$ods_intervention, # Talkworks, Devon
  ) |>
  ggplot2::ggplot(
    mapping = ggplot2::aes(
      x = calc_month,
      y = o1_denom_discharges_count
    )
  ) +
  # add an arrow and text pointing to start of regular activity
  ggplot2::annotate(
    geom = "segment",
    x = zoo::as.yearmon("Apr 2021"),
    xend = zoo::as.yearmon("Nov 2021"),
    y = 900,
    yend = 900,
    colour = "#5881c1",
    arrow = ggplot2::arrow(type = "closed", length = ggplot2::unit(0.2, "cm"))
  ) +
  ggplot2::annotate(
    geom = "text",
    x = zoo::as.yearmon("Mar 2021"),
    y = 900,
    label = "Discharges begin",
    hjust = 1,
    colour = "#5881c1"
  ) +
  # add in the rest of the plot
  ggplot2::geom_line() +
  ggplot2::geom_point() +
  ggplot2::theme_minimal(base_size = 20) +
  ggplot2::theme(axis.title.x = ggplot2::element_blank()) +
  ggplot2::labs(
    title = "Talkworks, Devon",
    subtitle = "Referral discharges begin with regular frequency from January 2022",
    # x = "",
    y = "Number of discharges"
  )
```

Focusing on the period January 2022 to June 2025, the time series for each of the outcome variables is shown below. Each plot includes a trendline in [orange]{style="color: #f9bf07;"} and a dotted line in [blue]{style="color: #5881c1;"} indicating when each of the interventions was started.

```{r}
#| label: Devon outcome 1
#| fig-height: 7
#| cache: true

# filter the data for Devon and a suitable timeframe
df_temp <-
  df |>
  dplyr::filter(
    ods_code == params$ods_intervention,
    dplyr::between(
      x = calc_month,
      left = zoo::as.yearmon("Jan 2022"),
      right = zoo::as.yearmon("Jun 2025")
    )
  )

# get a rate of increase for Devon's outcome 1
lm_temp <-
  lm(
    formula = o1_rate ~ calc_month,
    data = df_temp
  )

# extract as a variable
rate_per_year <-
  lm_temp |>
  broom::tidy() |>
  dplyr::filter(term == "calc_month") |>
  dplyr::pull(estimate) |>
  round(3)

# plot
df_temp |>
  plot_outcomes_over_time(
    df_interventions = df_interventions,
    var_outcome = "o1_rate",
    trendline = TRUE,
    interventions = TRUE,
    title = "Talkworks Devon",
    subtitle = stringr::str_wrap(
      string = glue::glue(
        "Outcome 1 shows an overall upward trend between 2022 and 2025, at a rate of {scales::percent(rate_per_year, accuracy = 0.1)} per year"
      ),
      width = 50
    )
  )
```

```{r}
#| label: Devon outcome 2
#| fig-height: 7
#| cache: true

# get a rate of increase for Devon's outcome 1
lm_temp <-
  lm(
    formula = o2_rate ~ calc_month,
    data = df_temp
  )

# extract as a variable
rate_per_year <-
  lm_temp |>
  broom::tidy() |>
  dplyr::filter(term == "calc_month") |>
  dplyr::pull(estimate) |>
  round(3)

# plot
df_temp |>
  plot_outcomes_over_time(
    df_interventions = df_interventions,
    var_outcome = "o2_rate",
    trendline = TRUE,
    interventions = TRUE,
    title = "Talkworks Devon",
    subtitle = stringr::str_wrap(
      string = glue::glue(
        "Outcome 2 shows an overall upward trend between 2022 and 2025, at a rate of {scales::percent(rate_per_year, accuracy = 0.1)} per year"
      ),
      width = 50
    )
  )
```

The matching process will use data from the eleven-month period preceding Talkworks Devon's first intervention, specifically from January 2022 to November 2022. This pre-intervention period provides a stable baseline for matching services, ensuring that the comparison is conducted before any intervention effects could potentially influence the data.

# National outcome trends

We will first look at the antional trends for our two outcome measures. Think of the national trajectory as a benchmark: Devon's interventions need to achieve improvements **faster than** this baseline in order to demonstrate measurable impact.

The next two charts include only months in which each services recorded at least 100 discharges. This filter removes unreliable rates that can arise from newly launched services or months with very few discharges.

```{r}
#| fig-height: 7
#| label: outcome 1 national trend
#| cache: true

# filter the data to make it easier to use
df_temp <-
  df |>
  dplyr::filter(
    # limit to the start of Devon's activity and exclude latest point
    dplyr::between(
      calc_month,
      zoo::as.yearmon("Jan 2022"),
      zoo::as.yearmon("May 2025")
    ),
    # limit to months where each service has more than 100 discharges to avoid spurious rates
    o1_denom_discharges_count > 100
  )

# linear model
lm_temp <-
  lm(
    formula = o1_rate ~ calc_month,
    data = df_temp
  )
# extract as a variable
rate_per_year <- lm_temp |>
  broom::tidy() |>
  dplyr::filter(term == "calc_month") |>
  dplyr::pull(estimate) |>
  round(3)

# get average rates per month
df_summary_per_month <-
  df_temp |>
  dplyr::summarise(
    o1_rate_av = mean(o1_rate, na.rm = TRUE),
    o1_rate_sd = sd(o1_rate, na.rm = TRUE),
    o2_rate_av = mean(o2_rate, na.rm = TRUE),
    o2_rate_sd = sd(o2_rate, na.rm = TRUE),
    .by = c(calc_month)
  ) |>
  dplyr::arrange(calc_month)

# show a spaghetti plot of all TT services over time
ggplot2::ggplot() +
  # add traces for each individual TT service
  ggplot2::geom_line(
    data = df_temp,
    mapping = ggplot2::aes(
      x = calc_month,
      y = o1_rate,
      group = ods_code
    ),
    colour = adjustcolor("#9d928a", alpha.f = 0.3),
    linewidth = 0.5
  ) +
  # add a ribbon of +/- one sd to show spread
  ggplot2::geom_ribbon(
    data = df_summary_per_month,
    mapping = ggplot2::aes(
      x = calc_month,
      ymin = (o1_rate_av - o1_rate_sd),
      ymax = (o1_rate_av + o1_rate_sd)
    ),
    fill = adjustcolor("#f9bf07", alpha.f = 0.2)
  ) +
  # add a trendline
  ggplot2::geom_smooth(
    data = df_temp,
    mapping = ggplot2::aes(x = calc_month, y = o1_rate),
    method = "lm",
    formula = y ~ x, # made explicit to avoid console messages
    se = FALSE,
    colour = adjustcolor("#2c2825", alpha.f = 1),
    linetype = "dotted"
  ) +
  ggplot2::scale_y_continuous(labels = scales::label_percent(accuracy = 1)) +
  zoo::scale_x_yearmon() +
  ggplot2::theme_minimal(base_size = 20) +
  ggplot2::theme(
    axis.title = ggplot2::element_blank()
  ) +
  ggplot2::labs(
    title = "Outcome 1: discharges with 5+ treatment contacts",
    subtitle = glue::glue(
      "National trend since 2022 is upwards at a rate of {rate_per_year * 100}% per year"
    )
  )
```

**What the chart shows:**

-   Each grey line traces a single Talking Therapy service's monthly performance on outcome 1 (percentage of dishcarged referrals that received at least five treatment contacts).

-   The dotted line is a linear regression that captures the overall trend across all services.

-   The yellow band around the dotted line marks ± 1 standard deviation, illustrating the typical spread of service-level rates.

Nationally, the proportion of referrals meeting the 5-contact threshold has risen by around 1.6% per year, climbing from \~57% in Jan 2022 to \~ 62% in May 2025. Monthly results are highly variable; some services approach 100% in certain months, while others fall to near 0%, especially during mid-to-late 2023 when several services achieved 0% in several months.

The upward national trend indicates overall improvement, but the wide dispersion (yellow band) shows that individual services experience very different outcomes, highlighting opportunities for targeted support.

```{r}
#| fig-height: 7
#| label: outcome 2 national trend
#| warning: false
#| cache: true

# linear model
lm_temp <-
  lm(
    formula = o2_rate ~ calc_month,
    data = df_temp
  )
# extract as a variable
rate_per_year <- lm_temp |>
  broom::tidy() |>
  dplyr::filter(term == "calc_month") |>
  dplyr::pull(estimate) |>
  round(3)

# show a spaghetti plot of all TT services over time
ggplot2::ggplot() +
  # add traces for each individual TT service
  ggplot2::geom_line(
    data = df_temp,
    mapping = ggplot2::aes(
      x = calc_month,
      y = o2_rate,
      group = ods_code
    ),
    colour = adjustcolor("#9d928a", alpha.f = 0.3),
    linewidth = 0.5
  ) +
  # add a ribbon of +/- one sd to show spread
  ggplot2::geom_ribbon(
    data = df_summary_per_month,
    mapping = ggplot2::aes(
      x = calc_month,
      ymin = (o2_rate_av - o2_rate_sd),
      ymax = (o2_rate_av + o2_rate_sd)
    ),
    fill = adjustcolor("#f9bf07", alpha.f = 0.2)
  ) +
  # add a trendline
  ggplot2::geom_smooth(
    data = df_temp,
    mapping = ggplot2::aes(x = calc_month, y = o2_rate),
    method = "lm",
    formula = y ~ x, # made explicit to avoid console messages
    se = FALSE,
    colour = adjustcolor("#2c2825", alpha.f = 1),
    linetype = "dotted"
  ) +
  ggplot2::scale_y_continuous(labels = scales::label_percent(accuracy = 1)) +
  zoo::scale_x_yearmon() +
  ggplot2::theme_minimal(base_size = 20) +
  ggplot2::theme(
    axis.title = ggplot2::element_blank()
  ) +
  ggplot2::labs(
    title = "Outcome 2: discharges achieving reliable recovery",
    subtitle = glue::glue(
      "National trend since 2022 is upwards at a rate of {rate_per_year * 100}% per year"
    )
  )
```

**What the chart shows:**

-   Each grey line traces a single Talking Therapy service's monthly performance on outcome 2 (percentage of dishcarged referrals that received between 2 and 4 treatment contacts).

-   The dotted line is a linear regression that captures the overall trend across all services.

-   The yellow band around the dotted line marks ± 1 standard deviation, illustrating the typical spread of service-level rates.

The national share of these referrals has been declining at a rate of 0.9% per year since 2022. Some services report low rates for this outcome each month, indicating most of their completed cases reach the 5-contact threshold. Other services show over 50% of completed referrals falling short of five contacts, highlighting a gap in treatment intensity.

In mid-to-late 2023 a few services recorded 100% of their completed referrals in this category for certain months, underscoring pronounced month-to-month fluctuations.

While the overall picture is improving, the wide spread among individual services suggests targeted interventions are needed to help the higher-percentage providers move more referrals toward the five-contact benchmark.

# Matching variables

The aim of matching is to pair our intervention Talking Therapies service, Talkworks Devon, with one more untreated services that have similar charactersitics, creating a comparison group that mimics a randomised experiment. This reduces bias and makes it easier to estimat the intervention's effect from observational data.

## Using all matching variables - Nov 2022

The dataset is cut off at November 2022, the month immediately before Devon launched its first intervention - a FAQ page for prospective clients. This pre-intervention slice will serve as the basis for the matching process.

In the initial matching run we include all matching variables and specify five nearest neighbour services. Using several candidates increases the chance of finding one service that meets the parallel-trends assumption.

```{r}
#| label: matching_all_vars
#| fig-height: 9
#| cache: true

# prepare the data
df_prep <-
  df |>
  # keep just the variables of interest
  dplyr::select(
    c(
      ods_code,
      name,
      calc_month,
      dplyr::contains("o1_denom"),
      dplyr::contains("_rate")
    )
  ) |>
  # flag records for the intervention service
  dplyr::mutate(
    flag_intervention = ods_code %in% params$ods_intervention
  ) |>
  dplyr::filter(
    # keep services that have activity up to June 2025
    max(calc_month, na.rm = TRUE) >= zoo::as.yearmon("Jun 2025"),
    # keep services that have activity in each of the matching yearmonths
    # all(yearmons_matching %in% calc_month),
    # exclude other services that have implemented an intervention
    (ods_code == params$ods_intervention | flag_intervention == 0),
    .by = ods_code
  ) |>
  # shorten the 'o1_denom' variable name
  dplyr::rename("o1_denom" = o1_denom_discharges_count) |>
  # limit to just November 2022 data
  dplyr::filter(calc_month == zoo::as.yearmon("Nov 2022"))

# string wrap variable names to better fit the plot
ls_labels_wrap <-
  purrr::modify_if(
    .x = ls_labels,
    .p = is.character,
    .f = ~ stringr::str_wrap(string = .x, width = 50)
  )

# set the matching formula
formula_matching <-
  formula(
    paste0(
      "flag_intervention ~ ",
      paste0(
        stringr::str_subset(
          string = names(df_prep),
          pattern = "^m.*_rate$"
        ),
        collapse = " + "
      )
    )
  )

# do the matching
obj_matches <-
  MatchIt::matchit(
    data = df_prep,
    formula = formula_matching,
    distance = "glm",
    method = "nearest",
    ratio = 5,
  )

# review the covariate balance
cobalt::love.plot(
  x = obj_matches,
  abs = TRUE,
  thresholds = 0.1,
  var.names = ls_labels_wrap,
  stars = "raw",
  drop.distance = TRUE
) +
  ggplot2::theme_minimal(base_size = 16) +
  ggplot2::scale_colour_manual(
    values = c("Unadjusted" = "#ec6555", Adjusted = "#5881c1")
  )

```

There are three warning messages displayed. The first two warning messages come from the propensity score matching process and likely indicate issues such as perfect separation of the data, or multicollinearity, which break key logistic regression assumptions. The third warning message comes from the process to visualise the model.

Reviewing the data reveals possible causes:

-   M14 Proportion of contacts conducted in English - the asterisk means the 'raw' score is displayed rather than absolute standardised mean differences the rest of the variables are displayed in

-   M16 Proportion of contacts conducted via Internet Enabled Therapy and M17 Proportion of contacts conducted one-to-one both show large standardised differences.

Clearly, using all these matching variables is problematic. Let's investigate further

## Examining matching variables

### Pre-matching assessment

First let us see the level of balance in the matching variables before any matching occurs:

```{r}
#| label: pre-matching covariate balance
#| cache: true

# set the matching formula
formula_matching <-
  formula(
    paste0(
      "flag_intervention ~ ",
      paste0(
        c(
          stringr::str_subset(string = names(df_prep), pattern = "^o.*_denom$"),
          stringr::str_subset(string = names(df_prep), pattern = "^m.*_rate$")
        ),
        collapse = " + "
      )
    )
  )

# check the balance prior to matching
set.seed(123)
match_pre <-
  MatchIt::matchit(
    formula = formula_matching,
    data = df_prep,
    method = NULL
    # distance = 'glm'
  ) |>
  summary()

t <-
  match_pre$sum.all |>
  tibble::as_tibble(rownames = "Matching variable") |>
  # exclude the distance metric - it isn't very useful here
  dplyr::filter(`Matching variable` != "distance") |>
  # drop the var. ratio and std. pair dist measures as they're not useful as the model doens't resolve
  dplyr::select(-c(`Var. Ratio`, `Std. Pair Dist.`)) |>
  gt::gt() |>
  gt::fmt_number(
    columns = gt::everything(),
    decimals = 4
  ) |>
  gt::data_color(
    columns = c(`Std. Mean Diff.`),
    method = "numeric",
    palette = "#f5b2aa",
    rows = abs(`Std. Mean Diff.`) < 0.1
  ) |>
  gt::tab_options(quarto.disable_processing = TRUE)
t
```

This table lists each matching variable together with its average value for Talkworks Devon (the treated group) and for all other donor services (the control group).

Our goal is to identify variables that show large differences, because those are the ones that the matching process can potentially improve. Variables with a standardised mean difference less than 0.1 are highlighted in pink. Variables which are uncoloured indicate where we could increase similarity between the treated and control groups.

Variables with less than 0.1 difference are less promising for improvement. In this case, `m2_rate` (Proportion of discharges for people aged under 26 years at referral) and `m14_rate` (M14 Proportion of contacts conducted in English).

Recommendation: drop `m2_rate` and `m14_rate` from the matching process as there is little scope to improve balance on these variables.

### Multicollinearity

Multicollinearity occurs when two or more matching variables are highly linearly related. Because such variables convey overlapping information, the matching process struggles to calculate reliable propensity scores.

In this section we identify strongly related matching variables using the Variance Inflation Factor (VIF). VIF measures how much a variable is linearly related to the others; higher values indicate stronger multicollinearity.

-   A VIF between 5 and 10 is often considered a warning sign. In the table below these are highlighted in yellow.
-   A VIF greater than 10 is strongly signals multicollinearity. Variables with such high VIF values should be excluded from the model. In the table below, any variable whose VIF exceeds 10 is highlighted in pink to flag a potential problem.

```{r}
#| label: multicollinearity - step 1
#| cache: true

# define which vars to include in this process
matching_vars <- c(
  stringr::str_subset(string = names(df_prep), pattern = "^o.*_denom$"),
  stringr::str_subset(string = names(df_prep), pattern = "^m.*_rate$")
)
# exclude those we don't want
matching_vars <- setdiff(matching_vars, c("m2_rate", "m14_rate"))
t <- get_vif_score_table(matching_vars = matching_vars, df = df_prep)
t
```

In this table, `m12_rate` (the proportion of contacts conducted face-to-face) has the highest VIF of 249, far exceeding the usual threshold.

**Next step:** evaluate the impact of removing `m12_rate` from the matching model, as its extreme multicollinearity may distort the matching process.

```{r}
#| label: multicollinearity - step 2
#| cache: true

# define which vars to include in this process
matching_vars <- c(
  stringr::str_subset(string = names(df_prep), pattern = "^o.*_denom$"),
  stringr::str_subset(string = names(df_prep), pattern = "^m.*_rate$")
)
# exclude those we don't want
matching_vars <- setdiff(matching_vars, c("m2_rate", "m14_rate", "m12_rate"))
t <- get_vif_score_table(matching_vars = matching_vars, df = df_prep)
t
```

After dropping `m12_rate`, the VIF table shows lower multicollinearity overall, but a few variables still exceed the VIF \> 10 threshold. The highest remaining VIF belongs to `m4_rate` (proportion of discharges for people whose gender is female).

**Next step:** remove `m4_rate` from the matching model and re-run the VIF analysis to determine whether the remaining values fall below the high-VIF threshold and to assess any changes.

```{r}
#| label: multicollinearity - step 3
#| cache: true
# define which vars to include in this process
matching_vars <- c(
  stringr::str_subset(string = names(df_prep), pattern = "^o.*_denom$"),
  stringr::str_subset(string = names(df_prep), pattern = "^m.*_rate$")
)
# exclude those we don't want
matching_vars <- setdiff(
  matching_vars,
  c("m2_rate", "m14_rate", "m12_rate", "m4_rate")
)
t <- get_vif_score_table(matching_vars = matching_vars, df = df_prep)
t
```

After excluding `m4_rate`, six variables still show VIF \> 10. The highest of these is `m8_rate` (proportion of discharges for people who had a referral-to-treatment time within six weeks).

**Next step:** drop `m8_rate` from the matching model and recompute the VIFs to verify whether the remaining variables now fall below the high-VIF threshold.

```{r}
#| label: multicollinearity - step 4
#| cache: true
# define which vars to include in this process
matching_vars <- c(
  stringr::str_subset(string = names(df_prep), pattern = "^o.*_denom$"),
  stringr::str_subset(string = names(df_prep), pattern = "^m.*_rate$")
)
# exclude those we don't want
matching_vars <- setdiff(
  matching_vars,
  c("m2_rate", "m14_rate", "m12_rate", "m4_rate", "m8_rate")
)
t <- get_vif_score_table(matching_vars = matching_vars, df = df_prep)
t
```

Removing `m8_rate` uncovered high VIF scores for ten matching covariates. The highest of these is for `m17_rate` (proportion of contacts conducted one-to-one).

**Next step:** remove `m17_rate` from the set of matching covariates and re-estimate the propensity scores and check whether the model stabilises.

```{r}
#| label: multicollinearity - step 5
#| cache: true
# define which vars to include in this process
matching_vars <- c(
  stringr::str_subset(string = names(df_prep), pattern = "^o.*_denom$"),
  stringr::str_subset(string = names(df_prep), pattern = "^m.*_rate$")
)
# exclude those we don't want
matching_vars <- setdiff(
  matching_vars,
  c("m2_rate", "m14_rate", "m12_rate", "m4_rate", "m8_rate", "m17_rate")
)
t <- get_vif_score_table(matching_vars = matching_vars, df = df_prep)
t
```

After dropping `m17_rate`, the VIF table shows lower multicollinearity overall, but nine variables still exceed the VIF \> 10 threshold. The highest remaining VIF belongs to `m9_rate` (proportion of discharges where there was a step-up in therapy).

**Next step:** remove `m9_rate`from the set of matching covariates and re-estimate the propensity scores and check whether the model stabilises.

```{r}
#| label: multicollinearity - step 6
#| cache: true
# define which vars to include in this process
matching_vars <- c(
  stringr::str_subset(string = names(df_prep), pattern = "^o.*_denom$"),
  stringr::str_subset(string = names(df_prep), pattern = "^m.*_rate$")
)
# exclude those we don't want
matching_vars <- setdiff(
  matching_vars,
  c(
    "m2_rate",
    "m14_rate",
    "m12_rate",
    "m4_rate",
    "m8_rate",
    "m17_rate",
    "m9_rate"
  )
)
t <- get_vif_score_table(matching_vars = matching_vars, df = df_prep)
t
```

After dropping `m9_rate`, the VIF table shows lower multicollinearity overall, but three variables still exceed the VIF \> 10 threshold. The highest remaining VIF belongs to `m13_rate` (proportion of contacts conducted outside of weekdays, 9am to 5pm).

**Next step:** drop `m13_rate` (proportion of contacts conducted outside of weekdays, 9am to 5pm) and re-run the model.

```{r}
#| label: multicollinearity - step 7
#| cache: true
# define which vars to include in this process
matching_vars <- c(
  stringr::str_subset(string = names(df_prep), pattern = "^o.*_denom$"),
  stringr::str_subset(string = names(df_prep), pattern = "^m.*_rate$")
)
# exclude those we don't want
matching_vars <- setdiff(
  matching_vars,
  c(
    "m2_rate",
    "m14_rate",
    "m12_rate",
    "m4_rate",
    "m8_rate",
    "m17_rate",
    "m9_rate",
    "m13_rate"
  )
)
t <- get_vif_score_table(matching_vars = matching_vars, df = df_prep)
t
```

Removing `m13_rate` uncovered high VIF scores for six matching covariates. The highest of these is for `m10_rate` (proportion of contacts where there was a qualified therapist present).

**Next step:** drop `m10_rate` and re-run the model.

```{r}
#| label: multicollinearity - step 8
#| cache: true
# define which vars to include in this process
matching_vars <- c(
  stringr::str_subset(string = names(df_prep), pattern = "^o.*_denom$"),
  stringr::str_subset(string = names(df_prep), pattern = "^m.*_rate$")
)
# exclude those we don't want
matching_vars <- setdiff(
  matching_vars,
  c(
    "m2_rate",
    "m14_rate",
    "m12_rate",
    "m4_rate",
    "m8_rate",
    "m17_rate",
    "m9_rate",
    "m13_rate",
    "m10_rate"
  )
)

t <- get_vif_score_table(matching_vars = matching_vars, df = df_prep)
t
```

After dropping `m10_rate`, the VIF table shows much lower multicollinearity overall, but two variables still exceed the VIF \> 10 threshold. The highest remaining VIF belongs to `m7_rate` (proportion of discharges for people with a *White* broad ethnic background).

**Next steps:** drop `m7_rate` and re-run the model.

```{r}
#| label: multicollinearity - step 9
#| cache: true
# define which vars to include in this process
matching_vars <- c(
  stringr::str_subset(string = names(df_prep), pattern = "^o.*_denom$"),
  stringr::str_subset(string = names(df_prep), pattern = "^m.*_rate$")
)
# exclude those we don't want
matching_vars <- setdiff(
  matching_vars,
  c(
    "m2_rate",
    "m14_rate",
    "m12_rate",
    "m4_rate",
    "m8_rate",
    "m17_rate",
    "m9_rate",
    "m13_rate",
    "m10_rate",
    "m7_rate"
  )
)

t <- get_vif_score_table(matching_vars = matching_vars, df = df_prep)
t
```

Removing `m7_rate` reduced the overall VIF scores down, leaving three matching variables with moderate VIFs and the model remains unstable. The variable with the highest VIF if `m16_rate` (proportion of contacts conducted via Internet Enabled Therapy).

**Next step:** drop `m16_rate` and re-run the model.

```{r}
#| label: multicollinearity - step 10
#| cache: true
# define which vars to include in this process
matching_vars <- c(
  stringr::str_subset(string = names(df_prep), pattern = "^o.*_denom$"),
  stringr::str_subset(string = names(df_prep), pattern = "^m.*_rate$")
)
# exclude those we don't want
matching_vars <- setdiff(
  matching_vars,
  c(
    "m2_rate",
    "m14_rate",
    "m12_rate",
    "m4_rate",
    "m8_rate",
    "m17_rate",
    "m9_rate",
    "m13_rate",
    "m10_rate",
    "m7_rate",
    "m16_rate"
  )
)

t <- get_vif_score_table(matching_vars = matching_vars, df = df_prep)
t
```

Removing `m16_rate` uncovered high VIF scores for four matching covariates. The highest of these is for `o1_denom` (number of discharges for referrals that were taken on for treatment).

**Next step:** drop `o1_denom` and re-run the model.

```{r}
#| label: multicollinearity - step 11
#| cache: true
# define which vars to include in this process
matching_vars <- c(
  stringr::str_subset(string = names(df_prep), pattern = "^o.*_denom$"),
  stringr::str_subset(string = names(df_prep), pattern = "^m.*_rate$")
)
# exclude those we don't want
matching_vars <- setdiff(
  matching_vars,
  c(
    "m2_rate",
    "m14_rate",
    "m12_rate",
    "m4_rate",
    "m8_rate",
    "m17_rate",
    "m9_rate",
    "m13_rate",
    "m10_rate",
    "m7_rate",
    "m16_rate",
    "o1_denom"
  )
)

t <- get_vif_score_table(matching_vars = matching_vars, df = df_prep)
t
```

All matching variables now have VIF scores below 2, confirming they are suitable for propensity-score matching. The model has converged, so the matching process proceeds without issue; the only remaining warning relates to having a single treated service and can safely be ignored.

The set of matching variables now consists of:

```{r}
#| label: multicollinearity - final list of variables
#| cache: true
# get the list of variables in a tibble

df_matching_var_labels <-
  tibble::tibble(
    variable = names(ls_labels),
    description = unlist(ls_labels, use.names = FALSE)
  ) |>
  dplyr::filter(variable %in% matching_vars)

# present as a {gt} table
df_matching_var_labels |>
  gt::gt() |>
  gt::tab_options(quarto.disable_processing = TRUE) |>
  gt::cols_label(
    variable = "Variable name",
    description = "Variable description"
  )
```

# Analysis

## Propensity score matching {#propensity-score-matching}

Using this list of matching variables we can now re-examine the propensity score matching results for Talkworks Devon.

The below plot is called a love plot (also known as a covariate balance plot) which visualises how well our matching variables are balanced between Talkworks Devon before and after applying a propensity-score matching process.

-   Our matching variables are listed along the y-axis and the x-axis shows the standardised mean difference (SMD), which is a measure of how different the averages are between the two groups.

-   Points in red indicate the differences between Devon and all other Talking Therapy services before matching. Points in blue indicate the differences between Devon and five of the closest matched services.

-   The dotted line is a reference at 0.2 SMD that indicates an acceptable imbalance threshold.

```{r}
#| label: propensity score matching new
#| fig-height: 7
#| message: false
#| warning: false
#| cache: true

# set the matching formula
formula_matching <-
  formula(
    paste0(
      "flag_intervention ~ ",
      paste0(matching_vars, collapse = " + ")
    )
  )

# do the matching
obj_matches <-
  MatchIt::matchit(
    data = df_prep,
    formula = formula_matching,
    distance = "glm",
    method = "nearest",
    ratio = 5,
  )

# review the covariate balance
cobalt::love.plot(
  x = obj_matches,
  abs = TRUE,
  thresholds = 0.2,
  var.names = ls_labels_wrap,
  stars = "raw",
  drop.distance = FALSE,
  var.order = "unadjusted"
) +
  ggplot2::theme_minimal(base_size = 16) +
  ggplot2::scale_colour_manual(
    values = c("Unadjusted" = "#ec6555", Adjusted = "#5881c1")
  )
```

These results show that the matched services are imperfect matches for Devon.

After matching, most variables are more similar between Devon and the five matched services (the blue points are nearer to 0 than the red points), despite this only two variables have blue points within the 0.2 Standardised Mean Difference (SMD) threshold.

The largest remaining imbalance concerns M3, the proportion of referrals for people aged 60 or older. The matching process reduced the difference, but the SMD still exceeds the 0.2 cut-off.

One variable, M6, became slightly more imbalanced after matching, however, their adjusted SMD is very close to the 0.2 SMD threshold.

The five matched services from this process are listed in the below table.

```{r}
#| label: propensity score matches
#| fig-height: 8
#| cache: true

# extract the matches
matches <- MatchIt::get_matches(obj_matches)

# show in a table
matches |>
  dplyr::select(name, ods_code) |>
  dplyr::mutate(
    ods_code = ods_code |>
      as.factor() |>
      forcats::fct_relevel(params$ods_intervention)
  ) |>
  dplyr::arrange(ods_code) |>
  gt::gt() |>
  gt::tab_options(quarto.disable_processing = TRUE) |>
  gt::data_color(
    columns = gt::everything(),
    rows = ods_code == params$ods_intervention,
    palette = "#f9bf07"
  ) |>
  gt::cols_label(
    name = "Talking Therapies organisation",
    ods_code = "ODS code"
  )
```

### Parallel trends assumption

The parallel trends assumption states that, if the intervention never occurred, Devon and its control group would have followed the same trajectory over time. A difference-in-differences (DiD) analysis depends on this assumption because it uses the control group's pre-intervention trend as the counterfactual for Devon.

To check this assumption holds we look at the pre-intervention period and if the outcome trends for both groups move together (i.e. have similar slopes), the assumption is plausible.

We will now assess the pre-intervention trends for Devon (shown in orange) and each of the five matched Talking Therapy services (shown in grey). Trendlines are overlaid on each plot to make it easier to judge whether the series move together.

#### Cambridge and Peterborough Voluntary Organisations RT1AB

**Outcome 1:** ❌ The control's trend is at 0% throughout and is not reliable

**Outcome 2:** ✔️ The trends are parallel

```{r}
#| label: PSM - cambridge
#| fig-height: 8
#| cache: true

# plots comparing each matched service with Devon
plot_list <-
  purrr::map(
    # select the matches - but exclude Devon
    .x = stringr::str_subset(
      string = matches$ods_code,
      pattern = params$ods_intervention,
      negate = TRUE
    ),
    .f = \(.x) {
      p <-
        compare_matches_preintervention(
          df = df,
          selected_ods = c(params$ods_intervention, .x),
          period_preintervention = zoo::as.yearmon(c("Jan 2022", "Nov 2022")),
          trendline = TRUE
        )
      return(p)
    }
  )

# display
plot_list[[1]]
```

#### Beacon Counselling DA6

**Outcome 1:** ❌ The trends are not parallel

**Outcome 2:** ❌ The trends are not parallel - they are convergent

```{r}
#| label: PSM - beacon
#| fig-height: 8
#| cache: true
plot_list[[2]]
```

#### Cornwall Partnership NHS Foundation Trust RJ8

**Outcome 1:** ❌ The trends are not parallel - they are slightly divergent

**Outcome 2:** ✔️ The trends are parallel

```{r}
#| label: PSM - cornwall
#| fig-height: 8
#| cache: true
plot_list[[3]]
```

#### Ealing Talking Therapies RKL07

**Outcome 1:** ✔️ The trends are parallel

**Outcome 2:** ✔️ The trends are parallel

```{r}
#| label: PSM - ealing
#| fig-height: 8
#| cache: true
plot_list[[4]]
```

#### Hertfordshire Partnership Foundation Trust (Tekhnicon House) RWRG3

**Outcome 1:** ✔️ The trends are parallel

**Outcome 2:** ❌ The trends are not parallel - they are divergent

```{r}
#| label: PSM - hertfordshire
#| fig-height: 8
#| cache: true
plot_list[[5]]
```

### Matching summary

Five matched Talking Therapy services were matched with Devon. One of these, *Ealing Talking Therapies* (RKL07), had pre-intervention trends that visually match those of Devon's for both outcome measures.

```{r}
#| label: PSM - matching summary
#| cache: true

# add these controls to a list for use in the synthetic control section
controls <- list(
  "psm_all" = matches$ods_code,
  "psm_parallel" = c("RKL07")
)
```

### Analysis

Now we have matched Devon with another Talking Therapy service and assessed their likelihood of meeting the parallel trends assumption based on their pre-intervention data, let us review how the full time series looks for this pair of services.

#### Outcome 1

```{r}
#| label: psm - spaghetti plot1 outcome 1
#| fig-height: 6
#| cache: true

# get a df with the details for our matched services
df_analysis <-
  df |>
  dplyr::filter(
    # limit to activity between Jan 2022 and May 2025
    dplyr::between(
      calc_month,
      zoo::as.yearmon("Jan 2022"),
      zoo::as.yearmon("May 2025")
    ),
    # limit to our matched services with parallel trends
    ods_code %in% c(params$ods_intervention, "RKL07")
  ) |>
  # simplify the data
  dplyr::select(ods_code, calc_month, o1_rate, o2_rate) |>
  # create treatment and post indicators
  dplyr::mutate(
    treated = dplyr::if_else(ods_code == params$ods_intervention, 1L, 0L),
    post = dplyr::if_else(calc_month >= zoo::as.yearmon("Dec 2022"), 1L, 0L),
    line_colour = dplyr::if_else(
      ods_code == params$ods_intervention,
      "#f9bf07",
      adjustcolor("#9d928a", alpha.f = 0.5)
    )
  )

# show a spaghetti plot for these services
p <-
  plot_spaghetti_plot(
    df = df_analysis,
    str_outcome = "o1_rate",
    ods_intervention = params$ods_intervention,
    str_title = "Outcome 1: discharges with 5+ treatment contacts",
    str_subtitle = "RKL07 displays abnormal results in mid-2023",
    zoo_intervention = zoo::as.yearmon("Dec 2022"),
    bool_intervention = TRUE
  )
p
```

**What this chart shows:**

-   Performance against outcome 1 is shown on the y-axis and time is along the x-axis.

-   There are two time series displayed, one for Devon, shown in orange, and one for our matched service, shown in grey.

-   Each series is identified by its ODS code at the end of the data in May 2025.

-   The dotted line represents the start of Devon's interventions in December 2022.

**Key observations:** The control services exhibit abnormal dips in Outcome 1 performance:

-   RKL07 fell from 70% in June 2023 to just 0% in July 2023

-   RKL07 remained lower than usual for August 2023 (32%) and September 2023 (48%) before returning to previous levels of performance in October 2023 (66%)

This brief, but steep decline would bias the post-implementation estimates upward for Devon. To avoid this bias, the low-performance points for RKL07 (July-September 2023) should be excluded from the dataset.

#### Outcome 2

```{r}
#| label: psm - spaghetti plot1 outcome 2
#| fig-height: 6
#| cache: true

# get a df with the details for our matched services
df_analysis <-
  df |>
  dplyr::filter(
    # limit to activity between Jan 2022 and May 2025
    dplyr::between(
      calc_month,
      zoo::as.yearmon("Jan 2022"),
      zoo::as.yearmon("May 2025")
    ),
    # limit to our matched services with parallel trends
    ods_code %in% c(params$ods_intervention, "RKL07")
  ) |>
  # simplify the data
  dplyr::select(ods_code, calc_month, o1_rate, o2_rate) |>
  # create treatment and post indicators
  dplyr::mutate(
    treated = dplyr::if_else(ods_code == params$ods_intervention, 1L, 0L),
    post = dplyr::if_else(calc_month >= zoo::as.yearmon("Dec 2022"), 1L, 0L),
    line_colour = dplyr::if_else(
      ods_code == params$ods_intervention,
      "#f9bf07",
      adjustcolor("#9d928a", alpha.f = 0.5)
    )
  )

# show a spaghetti plot for these services
p <-
  plot_spaghetti_plot(
    df = df_analysis,
    str_outcome = "o2_rate",
    ods_intervention = params$ods_intervention,
    str_title = "Outcome 2: discharges with reliable recovery",
    str_subtitle = "RKL07 displays abnormal results in mid-2023",
    zoo_intervention = zoo::as.yearmon("Dec 2022"),
    bool_intervention = TRUE
  )
p
```

**Key observations:** The control services exhibit abnormal dips in Outcome 1 performance:

-   RKL07 fell from 43% in June 2023 to just 15% in July 2023

-   RKL07 remained lower than usual for August 2023 (25%) and September 2023 (27%) before returning to previous levels of performance in October 2023 (40%)

This brief, but steep decline would bias the post-implementation estimates upward for Devon. To avoid this bias, the low-performance points for RKL07 (July-September 2023) should be excluded from the dataset.

#### Both outcomes excluding outliers

Here are the two outcomes excluding the anomalous months:

```{r}
#| label: psm - spaghetti plot2 outcomes 1 and 2
#| fig-height: 6
#| cache: true

# get a df excluding the problematic months for the controls
df_analysis2 <-
  df_analysis |>
  dplyr::filter(
    !(ods_code == "RKL07" &
      calc_month %in% zoo::as.yearmon(c("Jul 2023", "Aug 2023", "Sep 2023")))
  )

# show as a spaghetti plot
plot_spaghetti_plot(
  df = df_analysis2,
  str_outcome = "o1_rate",
  ods_intervention = params$ods_intervention,
  str_title = "Outcome 1: discharges with 5+ treatment contacts",
  str_subtitle = "These time series now seem less likely to bias the analysis",
  zoo_intervention = zoo::as.yearmon("Dec 2022"),
  bool_intervention = TRUE
)

# show as a spaghetti plot
plot_spaghetti_plot(
  df = df_analysis2,
  str_outcome = "o2_rate",
  ods_intervention = params$ods_intervention,
  str_title = "Outcome 2: discharges with reliable recovery",
  str_subtitle = "These time series now seem less likely to bias the analysis",
  zoo_intervention = zoo::as.yearmon("Dec 2022"),
  bool_intervention = TRUE
)
```

#### Difference-in-Differences (DiD)

The results of the DiD analyses are presented below:

```{r}
#| label: psm - did table
#| cache: true
#|
# conduct did analysis on the matched service
did <-
  get_manual_did_estimation(
    df = df_analysis2,
    ods_intervention = params$ods_intervention,
    zoo_intervention = zoo::as.yearmon("Dec 2022")
  )

# add to a combined summary (for sensitivity analysis)
did_overall <-
  did |>
  dplyr::mutate(specification = "Main model (baseline)")

# add details to a summary of ALL analyses in this document for selection for a subsequent meta analysis
did_meta <-
  tibble::tibble(
    specification = "PSM main model",
    outcome = did$outcome,
    estimate = did$estimate,
    se = did$std.error
  )

# display
did |>
  dplyr::select(
    outcome,
    estimate,
    conf.low,
    conf.high,
    p.value
  ) |>
  gt::gt() |>
  gt::tab_options(quarto.disable_processing = TRUE) |>
  gt::fmt_percent(
    columns = c(estimate, conf.low, conf.high),
    decimals = 2
  ) |>
  gt::cols_merge(
    columns = c(conf.low, conf.high),
    pattern = "{1} to {2}"
  ) |>
  gt::cols_label(
    outcome = "Outcome",
    estimate = "Difference-in-Differences (DiD) estimate",
    conf.low = "95% confidence interval"
  ) |>
  gt::tab_style(
    style = list(gt::cell_text(weight = "bold")),
    locations = gt::cells_body(
      columns = c(estimate, conf.low),
      rows = p.value <= 0.05
    )
  ) |>
  gt::cols_hide(columns = p.value) |>
  gt::tab_source_note(gt::md(
    "Statistically significant findings are shown in **bold**"
  ))
```

This table summarises the Difference-in-Differences analysis for Devon's interventions using the single control identified using the Propensity Score Matching process and which exhibited parallel trends.

For outcome 1, Devon's observed rate is about **2.4% higher** than what the matched controls would predict, yet the 95% confidence interval (-0.6% to 5.4%) includes zero, indicating **no statistically reliable difference**.

For outcome 2, Devon's rate is about **4.9% higher** than what the matched control would predict, and the 95% confidence interval (1.8% to 8.1%) indicates the effect is **statistically significant**.

::: callout-tip
## Interpretation

```{r}
#| label: interpretation help psm
#| cache: true

# work out the total discharges in 12 months preceeding
total_discharges <-
  df |>
  dplyr::filter(
    ods_code == params$ods_intervention,
    dplyr::between(
      calc_month,
      zoo::as.yearmon("Jun 2024"),
      zoo::as.yearmon("May 2025")
    )
  ) |>
  dplyr::summarise(
    total = sum(o1_denom_discharges_count, na.rm = TRUE)
  ) |>
  dplyr::pull(total)

# work out the expected number of people to benefit from the uplift
people_benefit_o1 <- floor(total_discharges * (2.37 / 100))
people_benefit_o2 <- floor(total_discharges * (4.93 / 100))
```

These findings indicate Devon's post-intervention performance exceeds the counterfactual for both outcomes, but the wide confidence interval in Outcome 1 is too wide to support a definitive claim of effectiveness meaning only Outcome 2 is statistically significant.

This means we have evidence that Devon's rate of people being discharged with reliable recovery is higher than expected, and this improvement cannot be wholly explained by a larger proportion of clients receiving five or more therapeutic contacts during referral.

Assuming the 4.9% gain is real, it translates to an additional **`r people_benefit_o2` people** achieving reliable recovery per year (based on Devon's annual discharge volume of `r scales::comma(total_discharges)` from June 2024 to May 2025).
:::

## Coarsened exact matching - Nov 2022

Coarsened Exact Matching (CEM) offers an alternative approach to propensity score matching. First, each variable is grouped into broader categories (e.g., 0-19%, 20-39%, 40-59%, etc.). Then Talking Therapy services are matched **exactly** on these coarsened bins.

The key advantage is that balance on the matching variables is guaranteed within each bin. The researcher controls the trade-off by choosing the width of the categories:

-   Narrower bins - tighter similarity between matched services but fewer matches

-   Wider bins - more matches but less precise similarity

The same matching variables selected from the multicollinearity optimisation process will be used. We will start with four cutpoints to divide each matching variable into five separate bins.

```{r}
#| label: coarsened exact matching
#| fig-height: 8
#| message: FALSE
#| cache: true

# do the matching
obj_matches <-
  MatchIt::matchit(
    data = df_prep,
    formula = formula_matching, # using the same formula as determined by VIF analysis
    method = "cem",
    cutpoints = 4
  )

# review the covariate balance
cobalt::love.plot(
  x = obj_matches,
  abs = TRUE,
  thresholds = 0.2,
  var.names = ls_labels_wrap,
  stars = "raw",
  var.order = "unadjusted"
) +
  ggplot2::theme_minimal(base_size = 16) +
  ggplot2::scale_colour_manual(
    values = c("Unadjusted" = "#ec6555", Adjusted = "#5881c1")
  )
```

As with the propensity score matching, these results show the matched services as close, but not perfect, matches for Devon.

After matching, every variable shows improved similarity (the blue points are nearer to 0 than the red points) and four variables are within the 0.2 SMD threshold, though three of these were already within this range prior to matching.

Applying four cut-points allowed the CEM procedure to match Devon with 20 Talking Therapy services. When we increased the number of cut-points to five - intended to boost sensitivity - only two matches were found.

The six matched services found using using four cut-points are:

```{r}
#| label: coarsened exact matches
#| fig-height: 8
#| cache: true

matches <- MatchIt::match_data(obj_matches)

# show in a table
matches |>
  dplyr::select(name, ods_code) |>
  dplyr::mutate(
    ods_code = ods_code |>
      as.factor() |>
      forcats::fct_relevel(params$ods_intervention)
  ) |>
  dplyr::arrange(ods_code) |>
  gt::gt() |>
  gt::tab_options(quarto.disable_processing = TRUE) |>
  gt::data_color(
    columns = gt::everything(),
    rows = ods_code == params$ods_intervention,
    palette = "#f9bf07"
  ) |>
  gt::cols_label(
    name = "Talking Therapies organisation",
    ods_code = "ODS code"
  )

# plots comparing each matched service with Devon
plot_list <-
  purrr::map(
    .x = stringr::str_subset(
      string = matches$ods_code,
      pattern = params$ods_intervention,
      negate = TRUE
    ),
    .f = \(.x) {
      p <-
        compare_matches_preintervention(
          df = df,
          selected_ods = c(params$ods_intervention, .x),
          period_preintervention = zoo::as.yearmon(c("Jan 2022", "Nov 2022")),
          trendline = TRUE
        )
      return(p)
    }
  )
```

### Parallel trends assumption

We now assess the parallel trends for Devon (shown in orange) and each of the 20 matched Talking Therapy services (shown in grey). Trendlines are overlaid on each plot to make it easier to judge whether the series move together.

#### Mind in Bexley (HQ) AME01

**Outcome 1:** ✔️ The trends are almost parallel, there is a hint of divergence but overall a close match

**Outcome 2:** ✔️ The trends are parallel

```{r}
#| label: cem - bexley
#| fig-height: 8
#| cache: true
plot_list[[1]]
```

#### Bikur Cholim Ltd DHA

**Outcome 1:** ❌ The trends are divergent

**Outcome 2:** ❌ The trends are convergent

```{r}
#| label: cem - bikur
#| fig-height: 8
#| cache: true
plot_list[[2]]
```

#### Vita Health Group: Vita Minds Basildon & Brentwood NWC05

**Outcome 1:** ✔️ The trends are almost parallel, there is a hint of divergence but overall a close match

**Outcome 2:** ✔️ The trends are parallel

```{r}
#| label: cem - basildon
#| fig-height: 8
#| cache: true
plot_list[[3]]
```

#### Vita Health Group: Vitaminds West Essex NWC08

**Outcome 1:** ❌ The trends are convergent

**Outcome 2:** ✔️ The trends are parallel

```{r}
#| label: cem - vita essex
#| fig-height: 8
#| cache: true
plot_list[[4]]
```

#### Vita Health Group: Vitaminds Derby NWC11

**Outcome 1:** ✔️ The trends are parallel

**Outcome 2:** ❌ The trends are convergent

```{r}
#| label: cem - derby
#| fig-height: 8
#| cache: true
plot_list[[5]]
```

#### Herefordshire and Worcestershire Health and Care NHS Trust R1A

**Outcome 1:** ✔️ The trends are parallel

**Outcome 2:** ✔️ The trends are parallel

```{r}
#| label: cem - herefordshire
#| fig-height: 8
#| cache: true
plot_list[[6]]
```

#### Essex Partnership University NHS Foundation Trust R1L

**Outcome 1:** ✔️ The trends are parallel

**Outcome 2:** ✔️ The trends are parallel

```{r}
#| label: cem - essex
#| fig-height: 8
#| cache: true
plot_list[[7]]
```

#### North East London NHS Foundation Trust RAT

**Outcome 1:** ✔️ The trends are parallel

**Outcome 2:** ✔️ The trends are parallel, though there is some hint of convergence

```{r}
#| label: cem - ne london
#| fig-height: 8
#| cache: true
plot_list[[8]]
```

#### Somerset NHS Foundation Trust RH5

**Outcome 1:** ✔️ The trends are parallel

**Outcome 2:** ✔️ The trends are parallel

```{r}
#| label: cem - somerset
#| fig-height: 8
#| cache: true
plot_list[[9]]
```

#### Cornwall Partnership NHS Foundation Trust RJ8

**Outcome 1:** ❌ The trends are not parallel

**Outcome 2:** ✔️ The trends are parallel

```{r}
#| label: cem - cornwall
#| fig-height: 8
#| cache: true
plot_list[[10]]
```

#### Ealing Talking Therapies RKL07

**Outcome 1:** ✔️ The trends are parallel

**Outcome 2:** ✔️ The trends are parallel

```{r}
#| label: cem - ealing
#| fig-height: 8
#| cache: true
plot_list[[11]]
```

#### Hounslow Talking Therapies RKL42

**Outcome 1:** ✔️ The trends are parallel

**Outcome 2:** ✔️ The trends are parallel

```{r}
#| label: cem - hounslow
#| fig-height: 8
#| cache: true
plot_list[[12]]
```

#### Norfolk and Suffolk NHS Foundation Trust RMY

**Outcome 1:** ✔️ The trends are parallel

**Outcome 2:** ❌ The trends are not parallel - they are convergent

```{r}
#| label: cem - norfolk
#| fig-height: 8
#| cache: true
plot_list[[13]]
```

#### Northamptonshire Healthcare NHS Foundation Trust RP1

**Outcome 1:** ❌ The trends are not parallel - they are divergent

**Outcome 2:** ✔️ The trends are parallel

```{r}
#| label: cem - northamptonshire
#| fig-height: 8
#| cache: true
plot_list[[14]]
```

#### Lincolnshire Partnership NHS Foundation Trust RP7

**Outcome 1:** ✔️ The trends are parallel - there is a hint of divergence but these are a close match

**Outcome 2:** ✔️ The trends are parallel

```{r}
#| label: cem - lincolnshire
#| fig-height: 8
#| cache: true
plot_list[[15]]
```

#### Sutton IAPT RQYPR

**Outcome 1:** ❌ The trends are not parallel - they are divergent

**Outcome 2:** ✔️ The trends are parallel

```{r}
#| label: cem - sutton
#| fig-height: 8
#| cache: true
plot_list[[16]]
```

#### Midlands Partnership University NHS Foundation Trust RRE

**Outcome 1:** ❌ The trends are not parallel - they are divergent

**Outcome 2:** ✔️ The trends are parallel

```{r}
#| label: cem - midlands
#| fig-height: 8
#| cache: true
plot_list[[17]]
```

#### Mill House RV3AR

**Outcome 1:** ✔️ The trends are parallel

**Outcome 2:** ✔️ The trends are parallel

```{r}
#| label: cem - mill house
#| fig-height: 8
#| cache: true
plot_list[[18]]
```

#### Sussex Partnership NHS Foundation Trust RX2

**Outcome 1:** ✔️ The trends are parallel

**Outcome 2:** ✔️ The trends are parallel

```{r}
#| label: cem - sussex
#| fig-height: 8
#| cache: true
plot_list[[19]]
```

#### The Briary Unit RX3YE

**Outcome 1:** ❌ The trends are not parallel - they are divergent

**Outcome 2:** ❌ The trends are not parallel - they are convergent

```{r}
#| label: cem - briary
#| fig-height: 8
#| cache: true
plot_list[[20]]
```

### Matching summary

A total of 20 Talking Therapy services were matched with Devon using *Coarsened Exact Matching (CEM)*. Of these, 11 had pre-intervention trends that visually match those of Devon's for both outcome measures.

```{r}
#| label: cem - matches with parallel trends
#| cache: true

# list out the matches with parallel pre-intervention trends
matches_parallel_ods <- c(
  "AME01",
  "NWC05",
  "R1A",
  "R1L",
  "RAT",
  "RH5",
  "RKL07",
  "RKL42",
  "RP7",
  "RV3AR",
  "RX2"
)

# add these controls to a list for use in the synthetic control section
controls <- c(
  controls,
  list(
    "cem_all" = matches$ods_code,
    "cem_parallel" = matches_parallel_ods
  )
)

# display these in a table
matches |>
  dplyr::filter(ods_code %in% matches_parallel_ods) |>
  dplyr::select(name, ods_code) |>
  dplyr::arrange(ods_code) |>
  gt::gt() |>
  gt::tab_options(quarto.disable_processing = TRUE) |>
  gt::cols_label(
    name = "Talking Therapies organisation",
    ods_code = "ODS code"
  )
```

### Analysis

Now we have matched Devon with 11 other Talking Therapy services and assessed their likelihood of meeting the parallel trends assumption based on their pre-intervention data, let us review how the full time series looks for this group of services.

#### Outcome 1

```{r}
#| label: cem - spaghetti plot1 outcome 1
#| fig-height: 6
#| cache: true

# get a df with the details for our matched services
df_analysis <-
  df |>
  dplyr::filter(
    # limit to activity between Jan 2022 and May 2025
    dplyr::between(
      calc_month,
      zoo::as.yearmon("Jan 2022"),
      zoo::as.yearmon("May 2025")
    ),
    # limit to our matched services with parallel trends
    ods_code %in% c(params$ods_intervention, matches_parallel_ods)
  ) |>
  # simplify the data
  dplyr::select(ods_code, calc_month, o1_rate, o2_rate) |>
  # create treatment and post indicators
  dplyr::mutate(
    treated = dplyr::if_else(ods_code == params$ods_intervention, 1L, 0L),
    post = dplyr::if_else(calc_month >= zoo::as.yearmon("Dec 2022"), 1L, 0L),
    line_colour = dplyr::if_else(
      ods_code == params$ods_intervention,
      "#f9bf07",
      adjustcolor("#9d928a", alpha.f = 0.5)
    )
  )

# show a spaghetti plot for these services
p <-
  plot_spaghetti_plot(
    df = df_analysis,
    str_outcome = "o1_rate",
    ods_intervention = params$ods_intervention,
    str_title = "Outcome 1: discharges with 5+ treatment contacts",
    str_subtitle = "Two services (RKL07 & RKL42) display abnormal results mid-2023",
    zoo_intervention = zoo::as.yearmon("Dec 2022"),
    bool_intervention = TRUE
  )
p
# p |> plotly::ggplotly()
```

**What this chart shows:**

-   Performance against outcome 1 is shown on the y-axis and time is along the x-axis.

-   There are three time series displayed, one for Devon, shown in orange, and one for each of the two of our matched services, shown in grey.

-   Each series is identified by its ODS code at the end of the data in May 2025.

-   The dotted line represents the start of Devon's interventions in December 2022.

**Key observations:** The control services exhibit abnormal dips in Outcome 1 performance:

-   RKL07 fell from 49% in June 2023 to just 0% in July 2023 and remained lower than typical until October 2023

-   RKL42 fell from 47% in July 2023 to just 12% in August 2023 but returned to typical levels in September 2023

The brief, but steep declines would bias the post-implementation estimates upward for Devon. To avoid this bias, the low-performance points for RKL07 (July to September 2023) and RKL42 (August 2023) should be excluded from the dataset.

#### Outcome 2

```{r}
#| label: cem - spaghetti plot1 outcome 2
#| fig-height: 6
#| cache: true

# show a spaghetti plot for outcome 2
p <-
  plot_spaghetti_plot(
    df = df_analysis,
    str_outcome = "o2_rate",
    ods_intervention = params$ods_intervention,
    str_title = "Outcome 2: discharges achieving reliable recovery",
    str_subtitle = "Milder outliers were observed among four services across 2023-2025",
    zoo_intervention = zoo::as.yearmon("Dec 2022"),
    bool_intervention = TRUE
  )
p
# p |> plotly::ggplotly()
```

**Key observations:**

The trends for this outcome shows fewer concerning observations than seen in Outcome 1. Although there are several sharp downward spiles in 2023-2025, they are less severe and recover more quickly:

-   RKL07 fell to 16% in July 2023, then rebounded to typical peformance by October 2023

-   RKL42 dropped to 22% in August 2023 and returned to normal the next month; a second dip to 28% occurred in July 2024, after which performance normalised

-   RV3AR declined to 26% in Apr 2024 recovering to typical levels the following month

-   RAT slipped to 28% in March 2025 and stayed below the usual range for the reaminder of the series

Overall, these outliers are less extreme than those in Outcome 1 and are unlikely to be materially affect the counterfactual estimate.

#### Both outcomes excluding outliers

Here are the two outcomes excluding the anomalous months:

```{r}
#| label: cem - spaghetti plot2 outcomes 1 and 2
#| fig-height: 6
#| cache: true

# get a df excluding the problematic months for the controls
df_analysis2 <-
  df_analysis |>
  dplyr::filter(
    !(ods_code == "RKL07" &
      calc_month %in% zoo::as.yearmon(c("Jul 2023", "Aug 2023", "Sep 2023"))),
    !(ods_code == "RKL42" & calc_month == zoo::as.yearmon("Aug 2023"))
  )

# show as a spaghetti plot
plot_spaghetti_plot(
  df = df_analysis2,
  str_outcome = "o1_rate",
  ods_intervention = params$ods_intervention,
  str_title = "Outcome 1: discharges with 5+ treatment contacts",
  str_subtitle = "These time series now seem less likely to bias the analysis",
  zoo_intervention = zoo::as.yearmon("Dec 2022"),
  bool_intervention = TRUE
)

# show as a spaghetti plot
plot_spaghetti_plot(
  df = df_analysis2,
  str_outcome = "o2_rate",
  ods_intervention = "RWV",
  str_title = "Outcome 2: discharges achieving reliable recovery",
  str_subtitle = "These time series now seem less likely to bias the analysis",
  zoo_intervention = zoo::as.yearmon("Dec 2022"),
  bool_intervention = TRUE
)
```

#### Difference-in-Differences (DiD)

The results of the DiD analyses are presented below:

```{r}
#| label: cem - did table
#| cache: true

# conduct did analysis on the two matched services
did <-
  get_manual_did_estimation(
    df = df_analysis2,
    ods_intervention = params$ods_intervention,
    zoo_intervention = zoo::as.yearmon("Dec 2022")
  )

# add to a combined summary (for sensitivity analysis)
did_overall <-
  did |>
  dplyr::mutate(specification = "Main model (baseline)")

# add details to a summary of ALL analyses in this document for selection for a subsequent meta analysis
did_meta <-
  dplyr::bind_rows(
    did_meta,
    tibble::tibble(
      specification = "CEM main model",
      outcome = did$outcome,
      estimate = did$estimate,
      se = did$std.error
    )
  )


# display
did |>
  dplyr::select(
    outcome,
    estimate,
    conf.low,
    conf.high,
    p.value
  ) |>
  gt::gt() |>
  gt::tab_options(quarto.disable_processing = TRUE) |>
  gt::fmt_percent(
    columns = c(estimate, conf.low, conf.high),
    decimals = 2
  ) |>
  gt::cols_merge(
    columns = c(conf.low, conf.high),
    pattern = "{1} to {2}"
  ) |>
  gt::cols_label(
    outcome = "Outcome",
    estimate = "Difference-in-Differences (DiD) estimate",
    conf.low = "95% confidence interval"
  ) |>
  gt::tab_style(
    style = list(gt::cell_text(weight = "bold")),
    locations = gt::cells_body(
      columns = c(estimate, conf.low),
      rows = p.value <= 0.05
    )
  ) |>
  gt::cols_hide(columns = p.value) |>
  gt::tab_source_note(gt::md(
    "Statistically significant findings are shown in **bold**"
  ))
```

This table summarises the Difference-in-Differences analysis for Devon's interventions using the 11 controls identified using the Coarsened Exact Matching process and which exhibited parallel trends.

For outcome 1, Devon's observed rate is about **1.5% higher** than what the matched controls would predict, yet the 95% confidence interval (1.2% to 8.6%) includes zero, indicating **no statistically reliable difference**.

For outcome 2, Devon's rate is about **3.0% higher** than expected, and the interval (1.3% to 4.6%) indicates the effect is **statistically significant**.

::: callout-tip
## Interpretation

```{r}
#| label: interpretation help cem
#| cache: true

# work out the total discharges in 12 months preceeding
total_discharges <-
  df |>
  dplyr::filter(
    ods_code == "RWV",
    dplyr::between(
      calc_month,
      zoo::as.yearmon("Jun 2024"),
      zoo::as.yearmon("May 2025")
    )
  ) |>
  dplyr::summarise(
    total = sum(o1_denom_discharges_count, na.rm = TRUE)
  ) |>
  dplyr::pull(total)

# work out the expected number of people to benefit from the uplift
people_benefit_o1 <- floor(total_discharges * (1.45 / 100))
people_benefit_o2 <- floor(total_discharges * (2.95 / 100))
```

These figures clearly present Devon's post-intervention performance exceeding the counterfactual based on the matched services for both outcomes, but the wide confidence interval in Outcome 1 prevents a definitive claim of effectiveness meaning only Outcome 2 is statistically significant.

Assuming the 3% gain is real, it translates to an additional **`r people_benefit_o2` people** achieving reliable recovery per year (based on Devon's annual discharge volume of `r scales::comma(total_discharges)` from June 2024 to May 2025).
:::

### Sensitivity analysis

Two important decisions were made that led to the above analytical findings:

1.  **Parallel trend selection** - We inspected the 20 matched controls series visually and excluded nine that did not follow Devon's pre-intervention trend.

2.  **Anomalous month removal** - For the remaining matched services we omitted several months for two services with extreme values, which yielded more stable control performance across outcomes.

Below we reassess the findings after relaxing each of these decisions to determine whether the original conclusions hold or are sensitive to these analytical choices.

#### Keeping anomalous months

What impact does keeping the extreme values in the time series have?

```{r}
#| label: cem - sensitivity - anomalous months
#| cache: true

# conduct did analysis on the two matched services
did <-
  get_manual_did_estimation(
    df = df_analysis,
    ods_intervention = params$ods_intervention,
    zoo_intervention = zoo::as.yearmon("Dec 2022")
  )

# add to a combined summary (for sensitivity analysis)
did_overall <-
  dplyr::bind_rows(
    did_overall,
    did |>
      dplyr::mutate(specification = "Alternative 1 - keep anomalous months")
  )

# add details to a summary of ALL analyses in this document for selection for a subsequent meta analysis
did_meta <-
  dplyr::bind_rows(
    did_meta,
    tibble::tibble(
      specification = "CEM alternative 1 - keep anomalous months",
      outcome = did$outcome,
      estimate = did$estimate,
      se = did$std.error
    )
  )


# show as a table
did |>
  dplyr::select(
    outcome,
    estimate,
    conf.low,
    conf.high,
    p.value
  ) |>
  gt::gt() |>
  gt::tab_options(quarto.disable_processing = TRUE) |>
  gt::fmt_percent(
    columns = c(estimate, conf.low, conf.high),
    decimals = 2
  ) |>
  gt::cols_merge(
    columns = c(conf.low, conf.high),
    pattern = "{1} to {2}"
  ) |>
  gt::cols_label(
    outcome = "Outcome",
    estimate = "Difference-in-Differences (DiD) estimate",
    conf.low = "95% confidence interval"
  ) |>
  gt::tab_style(
    style = list(gt::cell_text(weight = "bold")),
    locations = gt::cells_body(
      columns = c(estimate, conf.low),
      rows = p.value <= 0.05
    )
  ) |>
  gt::cols_hide(columns = p.value) |>
  gt::tab_source_note(gt::md(
    "Statistically significant findings are shown in **bold**"
  ))
```

For outcome 1, Devon's observed rate is **2% higher** than what the matched controls would predict, and the 95% confidence interval (0.3% to 3.8%) indicates the effect is **statistically significant**.

For outcome 2, Devon's observed rate is **3.2% higher** than what the matched controls would predict, and the 95% confidence interval (1.5% to 4.9%) indicates the effect is **statistically significant**.

#### Using all CEM controls

What impact does using all 20 matched control services have on the findings?

```{r}
#| label: cem - sensitivity - all six controls
#| cache: true

# get a df with the details for our matched services
df_analysis <-
  df |>
  dplyr::filter(
    # limit to activity between Jan 2022 and May 2025
    dplyr::between(
      calc_month,
      zoo::as.yearmon("Jan 2022"),
      zoo::as.yearmon("May 2025")
    ),
    # limit to our matched services
    ods_code %in% c(params$ods_intervention, controls$cem_all)
  ) |>
  # simplify the data
  dplyr::select(ods_code, calc_month, o1_rate, o2_rate) |>
  # create treatment and post indicators
  dplyr::mutate(
    treated = dplyr::if_else(ods_code == params$ods_intervention, 1L, 0L),
    post = dplyr::if_else(calc_month >= zoo::as.yearmon("Dec 2022"), 1L, 0L),
    line_colour = dplyr::if_else(
      ods_code == params$ods_intervention,
      "#f9bf07",
      adjustcolor("#9d928a", alpha.f = 0.5)
    )
  )

# conduct did analysis on the two matched services
did <-
  get_manual_did_estimation(
    df = df_analysis,
    ods_intervention = params$ods_intervention,
    zoo_intervention = zoo::as.yearmon("Dec 2022")
  )

# add to a combined summary (for sensitivity analysis)
did_overall <-
  dplyr::bind_rows(
    did_overall,
    did |>
      dplyr::mutate(specification = "Alternative 2 - use all 20 controls")
  )

# add details to a summary of ALL analyses in this document for selection for a subsequent meta analysis
did_meta <-
  dplyr::bind_rows(
    did_meta,
    tibble::tibble(
      specification = "CEM alternative 2 - use all 20 controls",
      outcome = did$outcome,
      estimate = did$estimate,
      se = did$std.error
    )
  )


# display as a table
did |>
  dplyr::select(
    outcome,
    estimate,
    conf.low,
    conf.high,
    p.value
  ) |>
  gt::gt() |>
  gt::tab_options(quarto.disable_processing = TRUE) |>
  gt::fmt_percent(
    columns = c(estimate, conf.low, conf.high),
    decimals = 2
  ) |>
  gt::cols_merge(
    columns = c(conf.low, conf.high),
    pattern = "{1} to {2}"
  ) |>
  gt::cols_label(
    outcome = "Outcome",
    estimate = "Difference-in-Differences (DiD) estimate",
    conf.low = "95% confidence interval"
  ) |>
  gt::tab_style(
    style = list(gt::cell_text(weight = "bold")),
    locations = gt::cells_body(
      columns = c(estimate, conf.low),
      rows = p.value <= 0.05
    )
  ) |>
  gt::cols_hide(columns = p.value) |>
  gt::tab_source_note(gt::md(
    "Statistically significant findings are shown in **bold**"
  ))
```

For outcome 1, Devon's observed rate is **0.9% higher** than what the matched controls would predict, yet the interval (-0.7% to 2.5%) also includes zero, indicating **no statistically reliable difference**.

For outcome 2, Devon's observed rate is **2.7% higher** than what the matched controls would predict, and the 95% confidence interval (1% to 4.3%) indicates the effect is **statistically significant**.

#### Summary

```{r}
#| label: cem - sensitivity summary
#| cache: true

# display a summary table
did_overall |>
  # simplify
  dplyr::select(
    specification,
    outcome,
    did = estimate,
    p.value,
    conf.low,
    conf.high
  ) |>
  # group by the sensitivity test
  dplyr::group_by(specification) |>
  gt::gt(row_group_as_column = TRUE) |>
  gt::tab_options(quarto.disable_processing = TRUE) |>
  gt::fmt_percent(columns = c(did, conf.low, conf.high), decimals = 2) |>
  gt::cols_merge(columns = c(conf.low, conf.high), pattern = "{1} to {2}") |>
  gt::tab_style(
    style = list(gt::cell_text(weight = "bold")),
    locations = gt::cells_body(
      columns = c(did, conf.low),
      rows = p.value <= 0.05
    )
  ) |>
  gt::cols_hide(columns = p.value) |>
  gt::cols_label(
    outcome = "Outcome",
    did = "DiD estimate",
    conf.low = "95% confidence interval"
  ) |>
  gt::tab_source_note(gt::md(
    "Statistically significant findings are shown in **bold**"
  ))
```

These sensitivity tests **re-inforce** the main findings.

*When the anomalous months are retained*, the estimated treatment effect becomes larger, reaching statistical significance for both outcomes. The extreme spikes in the control series pull the counterfactual down for both outcomes, thereby exaggerating the gap between the observed Devon trend and its predicted trend.

*When all 20 matched services are used as controls*, the results reflect the main findings. Adding more controls slightly reduces the treatment effect for both outcomes but Outcome 2 remains statistically significant. Some of the services did not follow parallel pre-intervention trends, which would explain the apparent dampened effect in Devon's favour.

The treatment effect on Outcome 2 appears robust, while the effect on Outcome 1 is contingent on sample composition.

## Synthetic Difference-in-Differences (DiD)

The preceeding section highlights the challenges of finding genuine control services that closely match Devon's profile and satisfy the parallel-trends assumption. Because the estimated treatment effect proved sensitive to several analytical choices, we turn to alternative method: synthetic controls.

Synthetic Difference-in-Differences (Synthetic DiD) blends two ideas: the traditional difference-in-differences method and the synthetic control technique.

-   First it builds a "synthetic" version of the control group by assigning weights to untreated units so that, before the intervention, this synthetic group looks just like the treated unit.

-   Then it compares the outcomes after the intervention between the treated unit and its synthetic counterpart. By creating a better-matched control, Synthetic DiD helps produce more reliable estimates of a treatment's effect, especially when the parallel-trend assumption is weak.

The [{synthdid}](https://synth-inference.github.io/synthdid/) package was used to conduct these Synthetic DiD analyses. This uses the method described by [@arkhangelsky2019] to combine the strengths of synthetic-control weighting with the traditional DiD framework. According to the paper this technique delivers unbiased treatment-effect estimates, even when the parallel-trends assumption is violated.

In this approach we will build a 'donor pool' of all Talking Therapy services to create a synthetic counterfactual that estimates how Devon would have performed without its interventions.

-   As with the earlier section, we first exclude any service that has its own adherence-improvement intervention

-   The {synthdid} package requires a balanced panel; every service in the donor pool must have data for every month from January 2022 through May 2025.

-   Consequently, any service with even a single missing month is dropped from the donor pool.

### Outcome 1

```{r}
#| label: synthdid - outcome 1
#| fig-height: 8
#| warning: false
#| cache: true

# set up for the did analysis
set.seed(12345)
.yearmon_period = zoo::as.yearmon(c("Jan 2022", "May 2025"))
.yearmon_intervention = zoo::as.yearmon("Dec 2022")

# get a dataset that is ready for synthetic DiD
df_synth <-
  prepare_df_for_synthdid(
    df = df,
    outcome = "o1_rate",
    yearmon_period = .yearmon_period,
    yearmon_intervention = .yearmon_intervention,
    ods_treated = params$ods_intervention,
    df_intervention_services = df_intervention_services
  )

# get the did estimate, confidence intervals and plot
did <-
  delegate_synthdid_analysis(
    df_synth = df_synth,
    .yearmon_intervention = .yearmon_intervention,
    .yearmon_period = .yearmon_period,
    .ods_treated = params$ods_intervention,
    str_treated = "Devon",
    summary_spec = "Main model (baseline)",
    summary_outcome = "Outcome 1",
    str_outcome = "o1_rate",
    labs_title = "Outcome 1: discharges with 5+ treatment contacts",
    labs_subtitle = "Devon performed 0.5% higher than expected, but the difference is not statistically significant"
  )

# add these findings to a combined summary (for sensitivity analysis)
did_overall <-
  did$did_summary

# add details to a summary of ALL analyses in this document for selection for a subsequent meta analysis
did_meta <-
  dplyr::bind_rows(
    did_meta,
    tibble::tibble(
      specification = "Synthdid main model (all controls)",
      outcome = "Outcome 1",
      estimate = as.numeric(did$did_estimate),
      se = did$did_se
    )
  )

# display the plot
did$did_plot
```

**What the chart displays**

+----------------------------+-------------------------------------------------------------------------------+------------------------------------------------+
| Element                    | Meaning                                                                       | Visual clue                                    |
+============================+===============================================================================+================================================+
| Axes                       | Outcome 1 performance (y-axis) vs time (x-axis)                               | Standard Cartesian axes                        |
+----------------------------+-------------------------------------------------------------------------------+------------------------------------------------+
| Devon's actual performance | Measured values for Devon                                                     | Orange solid line                              |
+----------------------------+-------------------------------------------------------------------------------+------------------------------------------------+
| Synthetic control          | Counterfactual series built from weighted pre-intervention data               | Blue solid line                                |
+----------------------------+-------------------------------------------------------------------------------+------------------------------------------------+
| Intervention start         | Point at which the Devon interventions began                                  | Vertical grey line (December 2022)             |
+----------------------------+-------------------------------------------------------------------------------+------------------------------------------------+
| Pre-intervention weights   | Relative contribution of each pre-intervention month to the synthetic control | Pink shapes in the lower-left corner           |
+----------------------------+-------------------------------------------------------------------------------+------------------------------------------------+
| Prallelogram overlay       | Visual comparison of the two trajectories                                     | Blue-topped edge = synthetic control path      |
|                            |                                                                               |                                                |
|                            |                                                                               | Dotted bottom edge = counterfactual Devon path |
|                            |                                                                               |                                                |
|                            |                                                                               | Orange line = Devon's actual trajectory        |
+----------------------------+-------------------------------------------------------------------------------+------------------------------------------------+

: Chart key

**How to read the parallelogram**

-   Top edge (blue) - trajectory of the synthetic control from the pre- to post-intervention period

-   Bottom edge (dotted) - what Devon's trend would have looked like without any interventions (the counterfactual)

-   Orange line - Devon's observed post-intervention trend.

Devon's orange line rises more sharply than the dotted counterfactual, indicating a positive impact of the interventions. The estimated gain is 0.5% above the expected value. The 95% confidence interval spans -7% to + 8%, meaning the true effect could be slightly negative or moderately positive. Consequently, the result is not statistically significant.

#### Contribution plot

```{r}
#| label: synthdid - outcome 1 - contribution plot
#| fig-height: 5
#| warning: false
#| cache: true

set.seed(12345)

synthdid::synthdid_units_plot(
  estimates = did$did_estimate,
  se.method = "placebo"
) +
  ggplot2::theme(axis.text.x = ggplot2::element_text(size = 5))
```

This plot shows:

-   The synthetic estimate as the black horizontal line

-   Each control Talking Therapy service as a point, where servicees that contribute more weight to the synthetic model shown in larger size

-   The grey horizontal lines represent the end points of a 95% confidence interval

A total of `r length(summary(did$did_estimate)$controls)` Talking Therapy services were used in the construction of this synthetic control.

### Outcome 2

```{r}
#| label: synthdid - outcome 2
#| fig-height: 8
#| warning: false
#| cache: true

set.seed(12345)

df_synth <-
  prepare_df_for_synthdid(
    df = df,
    outcome = "o2_rate",
    yearmon_period = .yearmon_period,
    yearmon_intervention = .yearmon_intervention,
    ods_treated = params$ods_intervention,
    df_intervention_services = df_intervention_services
  )

# get the did estimate, confidence intervals and plot
did <-
  delegate_synthdid_analysis(
    df_synth = df_synth,
    .yearmon_intervention = .yearmon_intervention,
    .yearmon_period = .yearmon_period,
    .ods_treated = params$ods_intervention,
    str_treated = "Devon",
    summary_spec = "Main model (baseline)",
    summary_outcome = "Outcome 2",
    str_outcome = "o2_rate",
    labs_title = "Outcome 2: discharges achieving reliable recovery",
    labs_subtitle = "Devon performed 3% better than expected, but the difference is not statistically significant"
  )

# add these findings to a combined summary (for sensitivity analysis)
did_overall <-
  dplyr::bind_rows(
    did_overall,
    did$did_summary
  )

# add details to a summary of ALL analyses in this document for selection for a subsequent meta analysis
did_meta <-
  dplyr::bind_rows(
    did_meta,
    tibble::tibble(
      specification = "Synthdid main model (all controls)",
      outcome = "Outcome 2",
      estimate = as.numeric(did$did_estimate),
      se = did$did_se
    )
  )

# display the plot
did$did_plot
```

**How to read the parallelogram**

-   Top edge (dotted) - what Devon's trend would have looked like without any interventions (the counterfactual)

-   Bottom edge (blue) - trajectory of the synthetic control from the pre- to post-intervention period

-   Orange line - Devon's observed post-intervention trend.

Devon's orange line rises much more sharply than the dotted counterfactual, indicating a large effect in the post-intervention period. The estimated difference is 3% above the expected value. The 95% confidence interval spans -6% to + 13%, meaning the true effect could be negative or positive. Consequently, the result is not statistically significant.

#### Contribution plot

```{r}
#| label: synthdid - outcome 2 - contribution plot
#| fig-height: 5
#| warning: false
#| cache: true

set.seed(12345)

synthdid::synthdid_units_plot(
  estimates = did$did_estimate,
  se.method = "placebo"
) +
  ggplot2::theme(axis.text.x = ggplot2::element_text(size = 5))
```

A total of `r length(summary(did$did_estimate)$controls)` Talking Therapy services were used in the construction of this synthetic control.

### Summary

```{r}
#| label: synthdid - summary table
#| cache: true

did_overall |>
  display_did_summary_in_gt(sensitivity_summary = FALSE)
```

::: callout-tip
## Interpretation

```{r}
#| label: interpretation help synthdid
#| cache: true

# work out the total discharges in 12 months preceeding
total_discharges <-
  df |>
  dplyr::filter(
    ods_code == params$ods_intervention,
    dplyr::between(
      calc_month,
      zoo::as.yearmon("Jun 2024"),
      zoo::as.yearmon("May 2025")
    )
  ) |>
  dplyr::summarise(
    total = sum(o1_denom_discharges_count, na.rm = TRUE)
  ) |>
  dplyr::pull(total)

# work out the expected number of people to benefit from the uplift
people_benefit_o1 <- floor(total_discharges * (0.5 / 100))
people_benefit_o2 <- floor(total_discharges * (3.3 / 100))
```

The charts clearly shows Devon's post-intervention performance exceeding the synthetic counterfactual, but the wide confidence interval prevents a definitive claim of effectiveness.

Assuming the 3.3% gain for Outcome 2 is real, it translates to an additional **`r people_benefit_o2` people** achieving reliable per year (based on Devon's annual discharge volume of `r scales::comma(total_discharges)` from June 2024 to May 2025).
:::

### Sensitivity analysis

In the previous analysis we supplied {synthdid} with the outcomes for *all* Talking Therapy services that were not involved in any known adherence-improving intervention. This full set served as the donor pool for constructing a counterfactual that best matches Devon's pre-intervention outcome trends.

We adopted this approach because synthetic-control methods require a reasonably large donor pool to generate a reliable weighted counterfactual. However, the [propensity score matching](#propensity-score-matching) and [coarsened exact matching](#coarsened-exact-matching-nov-2022) sections showed that the donor pool varies across our matching variables.

In this section we examine how restricting the donor pool influences the DiD estimates and assess whether our original conclusions remain robust or become sensitive to these restrictions.

#### Using matches from PSM

What impact does using the matches returned from the [propensity score matching (PSM)](#propensity-score-matching) section have?

```{r}
#| label: synthdid - sensitivity PSM - outcome 1
#| fig-height: 6
#| warning: false
#| cache: true

set.seed(12345)

df_synth <-
  prepare_df_for_synthdid(
    df = df,
    outcome = "o1_rate",
    yearmon_period = .yearmon_period,
    yearmon_intervention = .yearmon_intervention,
    ods_treated = params$ods_intervention,
    ods_controls = controls$psm_all,
    df_intervention_services = df_intervention_services
  )

# get the did estimate, confidence intervals and plot
did <-
  delegate_synthdid_analysis(
    df_synth = df_synth,
    .yearmon_intervention = .yearmon_intervention,
    .yearmon_period = .yearmon_period,
    .ods_treated = params$ods_intervention,
    str_treated = "Devon",
    summary_spec = "Alternative 1 - using PSM matches",
    summary_outcome = "Outcome 1",
    str_outcome = "o1_rate",
    labs_title = "Outcome 1: discharges with 5+ treatment contacts",
    labs_subtitle = "Devon performed 0.2% higher than expected, but the difference is not statistically significant"
  )

# add these findings to a combined summary (for sensitivity analysis)
did_overall <-
  dplyr::bind_rows(
    did_overall,
    did$did_summary
  )

# add details to a summary of ALL analyses in this document for selection for a subsequent meta analysis
did_meta <-
  dplyr::bind_rows(
    did_meta,
    tibble::tibble(
      specification = "Synthdid alternative 1 (PSM controls)",
      outcome = "Outcome 1",
      estimate = as.numeric(did$did_estimate),
      se = did$did_se
    )
  )

# display the plot
did$did_plot
```

Devon performs **0.2% higher** than the synthetic control expectation, however, the 95% confidence interval (-4% to 5%) includes zero so we cannot make any definitive claim. The synthetic controls exhibits considerable variability both before and after the intervention, which indicates the model struggled to find an adequate counterfactual and adds uncertainty to the interpretation of the results.

```{r}
#| label: synthdid - sensitivity PSM - outcome 2
#| fig-height: 6
#| warning: false
#| cache: true

set.seed(12345)

df_synth <-
  prepare_df_for_synthdid(
    df = df,
    outcome = "o2_rate",
    yearmon_period = .yearmon_period,
    yearmon_intervention = .yearmon_intervention,
    ods_treated = params$ods_intervention,
    ods_controls = controls$psm_all,
    df_intervention_services = df_intervention_services
  )

# get the did estimate, confidence intervals and plot
did <-
  delegate_synthdid_analysis(
    df_synth = df_synth,
    .yearmon_intervention = .yearmon_intervention,
    .yearmon_period = .yearmon_period,
    .ods_treated = params$ods_intervention,
    str_treated = "Devon",
    summary_spec = "Alternative 1 - using PSM matches",
    summary_outcome = "Outcome 2",
    str_outcome = "o2_rate",
    labs_title = "Outcome 2: discharges achieving reliable recovery",
    labs_subtitle = "Devon performed 6% higher than expected"
  )

# add these findings to a combined summary (for sensitivity analysis)
did_overall <-
  dplyr::bind_rows(
    did_overall,
    did$did_summary
  )

# add details to a summary of ALL analyses in this document for selection for a subsequent meta analysis
did_meta <-
  dplyr::bind_rows(
    did_meta,
    tibble::tibble(
      specification = "Synthdid alternative 1 (PSM controls)",
      outcome = "Outcome 2",
      estimate = as.numeric(did$did_estimate),
      se = did$did_se
    )
  )

# display the plot
did$did_plot
```

This analysis shows Devon performed **5.9% above** the synthetic control expectation, and the 95% confidence interval (5% to 7%) indicates this finding is statistically significant. As observed with Outcome 1, the counterfactual shows considerable variation which adds uncertainty to the interpretation of the results.

#### Using matches from CEM

What impact does using the matches returned from the [coarsened exact matching (CEM)](#coarsened-exact-matching-nov-2022) section have?

```{r}
#| label: synthdid - sensitivity CEM - outcome 1
#| fig-height: 6
#| warning: false
#| cache: true

set.seed(12345)

# get a did estimate
df_synth <-
  prepare_df_for_synthdid(
    df = df,
    outcome = "o1_rate",
    yearmon_period = .yearmon_period,
    yearmon_intervention = .yearmon_intervention,
    ods_treated = params$ods_intervention,
    ods_controls = controls$cem_all,
    df_intervention_services = df_intervention_services
  )

# get the did estimate, confidence intervals and plot
did <-
  delegate_synthdid_analysis(
    df_synth = df_synth,
    .yearmon_intervention = .yearmon_intervention,
    .yearmon_period = .yearmon_period,
    .ods_treated = params$ods_intervention,
    str_treated = "Devon",
    summary_spec = "Alternative 2 - using CEM matches",
    summary_outcome = "Outcome 1",
    str_outcome = "o1_rate",
    labs_title = "Outcome 1: discharges with 5+ treatment contacts",
    labs_subtitle = "Devon performed 0.2% better than expected, but the difference is not statistically significant"
  )

# add these findings to a combined summary (for sensitivity analysis)
did_overall <-
  dplyr::bind_rows(
    did_overall,
    did$did_summary
  )

# add details to a summary of ALL analyses in this document for selection for a subsequent meta analysis
did_meta <-
  dplyr::bind_rows(
    did_meta,
    tibble::tibble(
      specification = "Synthdid alternative 2 (CEM controls)",
      outcome = "Outcome 1",
      estimate = as.numeric(did$did_estimate),
      se = did$did_se
    )
  )

# display the plot
did$did_plot
```

Devon's observed outcome was **0.2% higher** than the synthetic control expectation. However, the 95% confidence interval (-8% to 8%) includes zero, so the result is not statistically distinguishable from no effect.

In contrast with the above section, the synthetic control exhibits minimal variability and tracks Devon's pre-intervention trend closely, which strengthens confidence in the overall comparison.

```{r}
#| label: synthdid - sensitivity CEM - outcome 2
#| fig-height: 6
#| warning: false
#| cache: true

set.seed(12345)

df_synth <-
  prepare_df_for_synthdid(
    df = df,
    outcome = "o2_rate",
    yearmon_period = .yearmon_period,
    yearmon_intervention = .yearmon_intervention,
    ods_treated = params$ods_intervention,
    ods_controls = controls$cem_all,
    df_intervention_services = df_intervention_services
  )

# get the did estimate, confidence intervals and plot
did <-
  delegate_synthdid_analysis(
    df_synth = df_synth,
    .yearmon_intervention = .yearmon_intervention,
    .yearmon_period = .yearmon_period,
    .ods_treated = params$ods_intervention,
    str_treated = "Devon",
    summary_spec = "Alternative 2 - using CEM matches",
    summary_outcome = "Outcome 2",
    str_outcome = "o2_rate",
    labs_title = "Outcome 2: discharges achieving reliable recovery",
    labs_subtitle = "Devon performed 3% better than expected, but the difference is not statistically significant"
  )

# add these findings to a combined summary (for sensitivity analysis)
did_overall <-
  dplyr::bind_rows(
    did_overall,
    did$did_summary
  )

# add details to a summary of ALL analyses in this document for selection for a subsequent meta analysis
did_meta <-
  dplyr::bind_rows(
    did_meta,
    tibble::tibble(
      specification = "Synthdid alternative 2 (CEM controls)",
      outcome = "Outcome 2",
      estimate = as.numeric(did$did_estimate),
      se = did$did_se
    )
  )

# display the plot
did$did_plot
```

Devon's observed outcome was **3.1% higher** than the synthetic control expectation. However, the 95% confidence interval (-3% to 9%) includes zero, so the result is not statistically distinguishable from no effect.

#### Using a June 2023 intervention date

Devon launche a therapy-adherence programme in December 2022, which ran to December 2023. In the main analyses we treated December 2022 as the start of the intervention and all subsequent months as the post-intervention period.

Here we re-define the intervention point to June 2023, the midpoint of the programme, and examine how this alternate timing affects the estimated impact.

```{r}
#| label: synthdid - sensitivity june 2023 - outcome 1
#| fig-height: 6
#| warning: false
#| cache: true

set.seed(12345)

# get a did estimate
df_synth <-
  prepare_df_for_synthdid(
    df = df,
    outcome = "o1_rate",
    yearmon_period = .yearmon_period,
    yearmon_intervention = zoo::as.yearmon("Jun 2023"),
    ods_treated = params$ods_intervention,
    df_intervention_services = df_intervention_services
  )

# get the did estimate, confidence intervals and plot
did <-
  delegate_synthdid_analysis(
    df_synth = df_synth,
    .yearmon_intervention = .yearmon_intervention,
    .yearmon_period = .yearmon_period,
    .ods_treated = params$ods_intervention,
    str_treated = "Devon",
    summary_spec = "Alternative 3 - using June 2023 intervention",
    summary_outcome = "Outcome 1",
    str_outcome = "o1_rate",
    labs_title = "Outcome 1: discharges with 5+ treatment contacts",
    labs_subtitle = "TBC"
  )

# add these findings to a combined summary (for sensitivity analysis)
did_overall <-
  dplyr::bind_rows(
    did_overall,
    did$did_summary
  )

# add details to a summary of ALL analyses in this document for selection for a subsequent meta analysis
did_meta <-
  dplyr::bind_rows(
    did_meta,
    tibble::tibble(
      specification = "Synthdid alternative 3 (June 2023 intervention)",
      outcome = "Outcome 1",
      estimate = as.numeric(did$did_estimate),
      se = did$did_se
    )
  )

# display the plot
did$did_plot
```

```{r}
#| label: synthdid - sensitivity june 2023 - outcome 2
#| fig-height: 6
#| warning: false
#| cache: true

set.seed(12345)

# get a did estimate
df_synth <-
  prepare_df_for_synthdid(
    df = df,
    outcome = "o2_rate",
    yearmon_period = .yearmon_period,
    yearmon_intervention = zoo::as.yearmon("Jun 2023"),
    ods_treated = params$ods_intervention,
    df_intervention_services = df_intervention_services
  )

# get the did estimate, confidence intervals and plot
did <-
  delegate_synthdid_analysis(
    df_synth = df_synth,
    .yearmon_intervention = .yearmon_intervention,
    .yearmon_period = .yearmon_period,
    .ods_treated = params$ods_intervention,
    str_treated = "Devon",
    summary_spec = "Alternative 3 - using June 2023 intervention",
    summary_outcome = "Outcome 2",
    str_outcome = "o2_rate",
    labs_title = "Outcome 2: discharges achieving reliable recovery",
    labs_subtitle = "TBC"
  )

# add these findings to a combined summary (for sensitivity analysis)
did_overall <-
  dplyr::bind_rows(
    did_overall,
    did$did_summary
  )

# add details to a summary of ALL analyses in this document for selection for a subsequent meta analysis
did_meta <-
  dplyr::bind_rows(
    did_meta,
    tibble::tibble(
      specification = "Synthdid alternative 3 (June 2023 intervention)",
      outcome = "Outcome 2",
      estimate = as.numeric(did$did_estimate),
      se = did$did_se
    )
  )

# display the plot
did$did_plot
```

#### Summary

```{r}
#| label: synthdid - sensitivity - summary table
#| cache: true
did_overall |>
  display_did_summary_in_gt(sensitivity_summary = TRUE)

# save a copy of the summary table for future meta-analyses
saveRDS(
  object = did_meta,
  file = here::here("data", "project", "did_estimates", "devon.Rds")
)
```

The sensitivity analyses suggest that the main results are broadly credible, but they also highlight where the evidence is fragile.

The main model, using all available control services to construct the synthetic control, found no statistically significant effect for either Outcome 1 or Outcome 2.

To assess the robustness to the choice of donor pool we re-ran the synthetic difference-in-differences analyses using the matched services identified using two methods of matching.

*Propensity score matching (PSM)* - When Devon is compared with the services selected by the PSM algorithm, the effect of Outcome 2 is found to be statistically significant.

*Coarsened exact matching (CEM)* - Using the CEM-derived donor pool reproduces the main findings: Devon tends to perform **slightly better** than its peers, but the confidence intervals are wide enough that we cannot reject the null hypothesis of no difference. CEM forces exact balance on key covariates, reducing heterogeneity among donor units.

Overall, the evidence points to a modest, possibly positive effect for Devon, but the result remains sensitive to donor-pool construction and the inherent noise in the synthetic control.

## Descriptive analysis

# Conclusions

We estimated the impact of Devon's interventions using three complimentary DiD approaches:

-   **Manual DiD** - Devon was compared with one services selected through a propensity score matched (PSM) procedure

-   **Manual DiD** - Devon was compared with 11 services selected through a coarsened exact matched (CEM) procedure

-   **Synthetic control DiD** - Devon was contrasted with a synthetic counterfactual build from all available donor services.

For the CEM and synthetic approaches we performed a series of sensitivity checks, for example, removing anomalous months, expanding the control pool, swapping CEM for PSM matches.

**No consistent, robust effect** was found across models for either outcome.

**Outcome 2 (proportion of discharges achieving reliable recovery)** showed statistically significant improvements in several models, suggesting a modest positive effect of Devon's interventions.

**Outcome 1 (proportion of discharges receiving 5 or more treatment contacts)** were not statistically significant in most models, though point estimates are slightly positive.

This evaluation of Devon's NHS Talking Therapies interventions - designed to improve therapy adherence - found **no robust or consistent evidence** of effectiveness across multiple analytical approaches.

While some methods, particularly those using the propensity score matching (PSM) and coarsened exact matching (CEM), showed **statistically significant improvements** in the proportion of patients who achieved reliable recovery on discharge (Outcome 2), these effects were **modest** and highly sensitive to methodological choices such as donor pool composition and the inclusion of anomalous months. For Outcome 1 (increasing the proportion of patients receiving five or more treatment sessions), results were generally positive but not statistically significant.

Overall, the results suggest a **possible signal of benefit**, but the evidence is not stable enough to support definitive conclusions.

**Possible signal**

The results from the manual DiD using PSM (point estimate \~ 2.4%), the manual DiD using CEM (point estimate \~ 1.5%) and the synthetic control DiD (point estimate 0.5%) point in the same direction, suggesting a modest positive effect. However, the confidence intervals are wide and overlap zero, so this 'hint' cannot be recorded as conclusive.

**Recommendations for future evaluation**

To obtain a definitive assessment of Devon's digital-resource and communication interventions, a more rigorous experimental design is needed:

1.  Identify a cohort of comparable services using the matching variables described in this report

2.  Randomly assign half of the services to receive the full intervention package (FAQ pages, explanatory animations, assessment video, communication review, DNA rate tracking) and the other half to serve as controls

3.  Track outcomes prospectively over and adequate post-intervention horizon

4.  Analyust the data with a pre-registered DiD or mixed-effects model, complemented by placebo tests and robustness checks

Such a randomised controlled design would eliminate the donor-pool and specification sensitivities that currently limit inference, allowing a clear determination of whether Devon's interventions truly improve client preparation and engagement.

# Limitations

The limitations for this analysis include:

**Only one treated unit**

The causal estimate rests on a single time-series (Devon). The result is that standard errors are large which translates to wide confidence intervals and a much larger treatment effect is required to reach statistical significance as a consequence.

**Possible hidden interventions in the donor pool**

We removed services with known interventions from the group of control services, but undisclosed changes may still be present.

Comparing Devon to other services that are also improving attenuates the estimated effect, biasing the results toward zero.

**Time-varying confounders**

Two control services experienced abrupt shocks in mid-2023 which distorted the post-implementation average performance and synthetic control trajectory, increased variability and weakened the credibility of the counterfactual.

**No single treatment date**

Devon's rollout was staggered across several components and spread over twelve months. As a result the "treated" period is diffuse, making it difficult to define a clean pre-/post-intervention split and to isolate the effect of any one component.

**Limited pre-intervention observations**

There are only 11 months of pre-treatment data versus 12 months of treatment and 18 months of post-treatment. Fewer pre-period points reduce confidence in the parallel-trend (or pre-fit) assumption and make the estimate more sensitive to short-term fluctuations.

**Synthetic control sensitivity to outliers**

The CEM-derived donor pool contained services with extreme spikes. Outliers can dominate the weighted synthetic control, producing unstable counterfactuals and potentially misleading effect sizes.

Take together, these limitations suggest that the estimated impacts should be interpreted with caution and highlight the need for a more rigorous, preferably experimental, design in future evaluations.